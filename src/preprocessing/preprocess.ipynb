{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603b2061",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39ff53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from scipy.fft import fft\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73689cd4",
   "metadata": {},
   "source": [
    "# Data Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5933b961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FONTOS: Minden event-re tamadaskent hivatkozunk (ettol fuggetlenul persze a nagy reszuk nem volt tamdas, csak magas forgalom)\n",
    "\n",
    "# Tomoren: A components tabla elemei az egyes tamadasok soran keszult 'snapshot'-ok a halozat allapotarol (egy tamadashoz akar tobb ilyen 'snapshot' is lehet)\n",
    "# Az events tabla elemei pedig egy-egy tamadast irnak le, igazabol az events tabla sorai az azonos Attack ID-val rendelkezo komponensek aggregacioja (atlaga, osszege, stb.)\n",
    "\n",
    "##### Components #####\n",
    "# Egy adott idopillanatban tukrozi halozat allapotat, egy snaphot\n",
    "# Az Attack ID azonositja, hogy melyik tamadasnak a resze az adot idopillanatbeli allapot, ebben a tablaban az Attack ID nem egyedi\n",
    "Components = {\n",
    "    \"Attack ID\": \"Egy adott tamadashoz (ami egy event) tartozo azonosito. Egy sora a tablanak azt mondja meg, hogy az adott tamadasnak az adott idopillanatban milyen allapotaban van a halozat.\",\n",
    "    \"Detect count\": \"Azt mondja meg, hogy a bejegyzes az adott Attack ID-ju tamadasnak hanyadik detektalasa. pl lehet, hogy egy tamadast csak egyszer detektalunk, de lehet, hogy a tamadas soran tobbszor is valami gyanusat erzekelt a detektor es ezeket mindet feljegyezte\",\n",
    "    \"Card\": \"A detektor melyik kartyaja vegezte az adatgyujtest\",\n",
    "    \"Victim IP\": \"A tamadas aldozatanak IP cime (anonimizalva)\",\n",
    "    \"Port number\": \"A tamadas cel portja\",\n",
    "    \"Attack code\": \"A tamadas jelleget irja le, amibol gyanus volt a detektornak, hogy tamadas tortenik. pl.: High volume / Suspicious traffic, CLDAP. Akar tobb is lehet egy sorban.\",\n",
    "    \"Significant flag\": \"A DDoS detektor belso flagje, SZAMUNKRA NEM FONTOS\",\n",
    "    \"Packet speed\": \"csomagráta (hálózaton áthaladó csomagok száma másodpercenként) [pps]\",\n",
    "    \"Data speed\": \"adatrata (hálózaton áthaladó adatmennyiség másodpercenként) [bps]\",\n",
    "    \"Avg packet len\": \"átlagos csomaghossz [byte]\",\n",
    "    \"Source IP count\": \"Egyedi IP cimek szama, akik az adott idopillanatban tamadnak\",\n",
    "    \"Time\": \"Timestamp, hogy mikor keszult a snapshot\"\n",
    "}\n",
    "\n",
    "\n",
    "##### Events #####\n",
    "# egy-egy tamadast reprezental\n",
    "# Azonos Attack ID-val rendelkezo komponensek aggregacioja, a teljes tamadast reprezentalja, az Attack ID egyedi\n",
    "Events = {\n",
    "    \"Attack ID\": \"ugyanaz, mint a Components tablaban, de itt egyedi\",\n",
    "    \"Card\": \"ugyanaz, mint a Components tablaban\",\n",
    "    \"Victim IP\": \"ugyanaz, mint a Components tablaban\",\n",
    "    \"Port number\": \"ugyanaz, mint a Components tablaban\",\n",
    "    \"Attack code\": \"uyanaz, mint a Components tablaban, az adott Attack ID-hoz tartozo osszes komponens Attack code-janak az osszessege\",\n",
    "    \"Detect count\": \"Hanyszor volt az adott Attack ID-val rendelkezo tamadas detektalva.\",\n",
    "    \"Singificant flag\": \"ugyanugy nem fontos\",\n",
    "    \"Packet speed\": \"Atlagos csomagráta a tamadas soran\",\n",
    "    \"Data speed\": \"Atlagos adatrata a tamadas soran\",\n",
    "    \"Avg packet len\": \"Atlagos csomaghossz a tamadas soran\",\n",
    "    \"Avg Source IP count\": \"Atlagososan hany IP-rol tortent a tamadas\",\n",
    "    \"Start time\": \"Timestamp, mikor kezdodott a tamadas\",\n",
    "    \"End time\": \"Timestamp, mikor vegezodott a tamadas\",\n",
    "    \"Whitelist flag\": \"Detektor belso flagje, SZAMUNKRA NEM FONTOS\",\n",
    "    \"Type\": \"Az esemeny kategoriaja, aminek a detektor felcimkezte, ezt kell majd nekunk prediktalni neuralis haloval. Itt ez a 'ground truth' adat. Lehetseges ertekei: DDoS attack, Suspicious traffic vagy Normal traffic\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e42d5",
   "metadata": {},
   "source": [
    "# Initial Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe19f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' We should do the feature engineering here. For the model training this will be exported to a proper DataClass'''\n",
    "\n",
    "component_columns = [\n",
    "    \"Attack ID\", \"Detect count\", \"Card\", \"Victim IP\", \"Port number\",\n",
    "    \"Attack code\", \"Significant flag\", \"Packet speed\", \"Data speed\", \"Avg packet len\",\n",
    "    \"Source IP count\", \"Time\"\n",
    "]\n",
    "\n",
    "event_columns = [\n",
    "    \"Attack ID\", \"Card\", \"Victim IP\", \"Port number\", \"Attack code\", \n",
    "    \"Detect count\", \"Significant flag\", \"Packet speed\", \"Data speed\", \n",
    "    \"Avg packet len\", \"Avg source IP count\", \"Start time\", \"End time\", \n",
    "    \"Whitelist flag\", \"Type\"\n",
    "]\n",
    "\n",
    "def get_fft(column, rel_times):\n",
    "    # If only one value is present, return zeros to avoid errors\n",
    "    if len(column) == 1:\n",
    "        return 0,0\n",
    "\n",
    "    values = column.values\n",
    "    N = len(values)\n",
    "    T = rel_times[-1] if rel_times[-1] > 0 else 1  # Time span in seconds\n",
    "\n",
    "    fft_vals = fft(values)\n",
    "    fft_mag = np.abs(fft_vals[:N // 2])  # Magnitudes\n",
    "    fft_freq = np.fft.fftfreq(N, T / N)[:N // 2]  # Frequencies\n",
    "\n",
    "    # Get the indices of the top magnitudes\n",
    "    num_mags = min(len(fft_mag), 3)  # Handle cases with <3 magnitudes\n",
    "    top_indices = np.argsort(fft_mag)[-num_mags:][::-1]  # Largest magnitudes in descending order\n",
    "\n",
    "    # Select corresponding frequencies and phases\n",
    "    top_magnitudes = fft_mag[top_indices].tolist()\n",
    "    top_frequencies = fft_freq[top_indices].tolist()\n",
    "\n",
    "    # Ensure the return values always have 3 elements\n",
    "    while len(top_magnitudes) < 3:\n",
    "        top_magnitudes.append(0)\n",
    "        top_frequencies.append(0)\n",
    "\n",
    "    return sum(top_magnitudes[1:]), sum(top_frequencies[1:])\n",
    "\n",
    "\n",
    "def process_components(events, components):\n",
    "    data = components.copy(deep=True)\n",
    "    data.columns = component_columns\n",
    "    \n",
    "    e_data = events.copy(deep=True)\n",
    "    e_data.columns = event_columns\n",
    "    \n",
    "    calculated_data = []\n",
    "    # Group by 'Attack ID' and iterate over each group\n",
    "        # Pre-convert 'Time' to datetime once\n",
    "    data[\"Time\"] = pd.to_datetime(data[\"Time\"])\n",
    "    \n",
    "    # List to hold calculated values for each group\n",
    "    calculated_data = []\n",
    "\n",
    "    # Group by 'Attack ID' and iterate over each group\n",
    "    grouped_data = data.groupby(\"Attack ID\")\n",
    "    for attack_id, group in tqdm(grouped_data):\n",
    "        group = group.sort_values(\"Detect count\")\n",
    "        # Calculate relative time using numpy (much faster than pandas)\n",
    "        times = group[\"Time\"].values\n",
    "        relative_time = np.cumsum(np.concatenate(([0], np.diff(times).astype(np.float64)))) / 10**9\n",
    "\n",
    "        # Calculate the detection frequency (detections per second) [Hz]\n",
    "        detection_frequency = 0 if relative_time[-1] == 0 else len(group) / relative_time[-1]\n",
    "\n",
    "        # Standard deviation in network parameters\n",
    "        packet_speed_std = group[\"Packet speed\"].std(ddof=0)\n",
    "        data_speed_std = group[\"Data speed\"].std(ddof=0)\n",
    "        avg_packet_len_std = group[\"Avg packet len\"].std(ddof=0)\n",
    "        source_ip_count_std = group[\"Source IP count\"].std(ddof=0)\n",
    "        \n",
    "        packet_speed_burst_ratio = group[\"Packet speed\"].max() / (group[\"Packet speed\"].median() + 1)\n",
    "        data_speed_burst_ratio = group[\"Data speed\"].max() / (group[\"Data speed\"].median() + 1)\n",
    "        avg_packet_len_burst_ratio = group[\"Avg packet len\"].max() / (group[\"Avg packet len\"].median() + 1)\n",
    "        source_ip_burst_ratio = group[\"Source IP count\"].max() / (group[\"Source IP count\"].median() + 1)\n",
    "            \n",
    "        # Measures how many packets are transmitted per unit of average packet length.\n",
    "        packet_transmission_rate = np.where(group['Avg packet len'] == 0, 0, group['Packet speed'] / group['Avg packet len']).max()\n",
    "        \n",
    "        # Measures how many packets are transmitted per unit of data speed.\n",
    "        packet_density = np.where(group['Data speed'] == 0, 0, group['Packet speed'] / group['Data speed']).max()   \n",
    "        \n",
    "        # Measures how much data (on average) is carried per unit of data speed. \n",
    "        packet_size_efficiency = np.where(group['Data speed'] == 0, 0, group['Avg packet len'] / group['Data speed']).max()        \n",
    "        \n",
    "\n",
    "\n",
    "        frequ_domain_values = []\n",
    "        for col in [\"Packet speed\", \"Data speed\", \"Avg packet len\", \"Source IP count\"]:\n",
    "            magnitudes, frequencies = get_fft(group[col], relative_time)\n",
    "            frequ_domain_values.append([magnitudes, frequencies])\n",
    "\n",
    "\n",
    "        # Collect calculated data for each attack\n",
    "        calculated_data.append({\n",
    "            \"Attack ID\": attack_id,\n",
    "            \"Detection Frequency\": detection_frequency,\n",
    "            \"Packet Speed Std\": packet_speed_std,\n",
    "            \"Data Speed Std\": data_speed_std,\n",
    "            \"Avg Packet Len Std\": avg_packet_len_std,\n",
    "            \"Source IP Count Std\": source_ip_count_std,\n",
    "            \"Packet Speed Burst Ratio\": packet_speed_burst_ratio,\n",
    "            \"Data Speed Burst Ratio\": data_speed_burst_ratio,\n",
    "            \"Avg Packet Len Burst Ratio\": avg_packet_len_burst_ratio,\n",
    "            \"Source IP Burst Ratio\": source_ip_burst_ratio,\n",
    "            \"Packet Transmission Rate\": packet_transmission_rate,\n",
    "            \"Packet Density\": packet_density,\n",
    "            \"Packet Size Efficiency\": packet_size_efficiency,\n",
    "            \"Packet Speed Ac Magnitude\": frequ_domain_values[0][0],\n",
    "            \"Packet Speed Ac Frequency\": frequ_domain_values[0][1],\n",
    "            \"Data Speed Ac Magnitude\": frequ_domain_values[1][0],\n",
    "            \"Data Speed Ac Frequency\": frequ_domain_values[1][1],\n",
    "            \"Avg Packet Len Ac Magnitude\": frequ_domain_values[2][0],\n",
    "            \"Avg Packet Len Ac Frequency\": frequ_domain_values[2][1],\n",
    "            \"Source IP Count Ac Magnitude\": frequ_domain_values[3][0],\n",
    "            \"Source IP Count Ac Frequency\": frequ_domain_values[3][1]\n",
    "        })\n",
    "        \n",
    "    # Convert the calculated data into a DataFrame\n",
    "    calculated_df = pd.DataFrame(calculated_data)\n",
    "\n",
    "    # Merge the calculated data back into the events DataFrame\n",
    "    e_data = pd.merge(e_data, calculated_df, on=\"Attack ID\", how=\"left\")\n",
    "    print(e_data.columns)\n",
    "    return e_data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def add_time_features(p_events):\n",
    "    events = p_events.copy(deep=True)\n",
    "    events = events[events['End time'].astype(str) != '0']\n",
    "    events['Start time'] = pd.to_datetime(events['Start time'])\n",
    "    events['End time'] = pd.to_datetime(events['End time'])\n",
    "    events['Duration'] = (events['End time'] - events['Start time']).dt.total_seconds().astype(int)\n",
    "    events = events.drop(columns=['Victim IP', 'Significant flag', 'Whitelist flag', 'Start time', 'End time', \"Card\", \"Attack code\", \"Attack ID\"])\n",
    "    cols = list(events.columns)\n",
    "    cols[6], cols[-1] = cols[-1], cols[6]\n",
    "    events = events[cols]\n",
    "    \n",
    "    # # 2. Hour of the day (optional: you can choose to store this directly or use booleans)\n",
    "    # events['start_hour'] = events['Start time'].dt.hour\n",
    "    # events['end_hour'] = events['End time'].dt.hour\n",
    "    \n",
    "    # 3. Boolean flags for each hour (this is optional, but if you'd like, here's an example)\n",
    "    # for hour in range(24):\n",
    "    #     events[f'is_hour_{hour}'] = events.apply(\n",
    "    #         lambda row: hour >= row['start_hour'] and hour <= row['end_hour'], axis=1\n",
    "    #     )\n",
    "    return events\n",
    "\n",
    "# Function to convert only numeric columns to float32\n",
    "def convert_to_float32(col):\n",
    "    # Check if the column is numeric\n",
    "    if pd.api.types.is_numeric_dtype(col):\n",
    "        return col.astype('float32')  # Convert to float32\n",
    "    else:\n",
    "        return col  # Return the column as is if not numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb044d6",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47364631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 134769/134769 [02:58<00:00, 753.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Attack ID', 'Card', 'Victim IP', 'Port number', 'Attack code',\n",
      "       'Detect count', 'Significant flag', 'Packet speed', 'Data speed',\n",
      "       'Avg packet len', 'Avg source IP count', 'Start time', 'End time',\n",
      "       'Whitelist flag', 'Type', 'Detection Frequency', 'Packet Speed Std',\n",
      "       'Data Speed Std', 'Avg Packet Len Std', 'Source IP Count Std',\n",
      "       'Packet Speed Burst Ratio', 'Data Speed Burst Ratio',\n",
      "       'Avg Packet Len Burst Ratio', 'Source IP Burst Ratio',\n",
      "       'Packet Transmission Rate', 'Packet Density', 'Packet Size Efficiency',\n",
      "       'Packet Speed Ac Magnitude', 'Packet Speed Ac Frequency',\n",
      "       'Data Speed Ac Magnitude', 'Data Speed Ac Frequency',\n",
      "       'Avg Packet Len Ac Magnitude', 'Avg Packet Len Ac Frequency',\n",
      "       'Source IP Count Ac Magnitude', 'Source IP Count Ac Frequency'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 129999/129999 [02:53<00:00, 748.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Attack ID', 'Card', 'Victim IP', 'Port number', 'Attack code',\n",
      "       'Detect count', 'Significant flag', 'Packet speed', 'Data speed',\n",
      "       'Avg packet len', 'Avg source IP count', 'Start time', 'End time',\n",
      "       'Whitelist flag', 'Type', 'Detection Frequency', 'Packet Speed Std',\n",
      "       'Data Speed Std', 'Avg Packet Len Std', 'Source IP Count Std',\n",
      "       'Packet Speed Burst Ratio', 'Data Speed Burst Ratio',\n",
      "       'Avg Packet Len Burst Ratio', 'Source IP Burst Ratio',\n",
      "       'Packet Transmission Rate', 'Packet Density', 'Packet Size Efficiency',\n",
      "       'Packet Speed Ac Magnitude', 'Packet Speed Ac Frequency',\n",
      "       'Data Speed Ac Magnitude', 'Data Speed Ac Frequency',\n",
      "       'Avg Packet Len Ac Magnitude', 'Avg Packet Len Ac Frequency',\n",
      "       'Source IP Count Ac Magnitude', 'Source IP Count Ac Frequency'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130000/130000 [02:55<00:00, 740.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Attack ID', 'Card', 'Victim IP', 'Port number', 'Attack code',\n",
      "       'Detect count', 'Significant flag', 'Packet speed', 'Data speed',\n",
      "       'Avg packet len', 'Avg source IP count', 'Start time', 'End time',\n",
      "       'Whitelist flag', 'Type', 'Detection Frequency', 'Packet Speed Std',\n",
      "       'Data Speed Std', 'Avg Packet Len Std', 'Source IP Count Std',\n",
      "       'Packet Speed Burst Ratio', 'Data Speed Burst Ratio',\n",
      "       'Avg Packet Len Burst Ratio', 'Source IP Burst Ratio',\n",
      "       'Packet Transmission Rate', 'Packet Density', 'Packet Size Efficiency',\n",
      "       'Packet Speed Ac Magnitude', 'Packet Speed Ac Frequency',\n",
      "       'Data Speed Ac Magnitude', 'Data Speed Ac Frequency',\n",
      "       'Avg Packet Len Ac Magnitude', 'Avg Packet Len Ac Frequency',\n",
      "       'Source IP Count Ac Magnitude', 'Source IP Count Ac Frequency'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 437657/437657 [09:49<00:00, 742.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Attack ID', 'Card', 'Victim IP', 'Port number', 'Attack code',\n",
      "       'Detect count', 'Significant flag', 'Packet speed', 'Data speed',\n",
      "       'Avg packet len', 'Avg source IP count', 'Start time', 'End time',\n",
      "       'Whitelist flag', 'Type', 'Detection Frequency', 'Packet Speed Std',\n",
      "       'Data Speed Std', 'Avg Packet Len Std', 'Source IP Count Std',\n",
      "       'Packet Speed Burst Ratio', 'Data Speed Burst Ratio',\n",
      "       'Avg Packet Len Burst Ratio', 'Source IP Burst Ratio',\n",
      "       'Packet Transmission Rate', 'Packet Density', 'Packet Size Efficiency',\n",
      "       'Packet Speed Ac Magnitude', 'Packet Speed Ac Frequency',\n",
      "       'Data Speed Ac Magnitude', 'Data Speed Ac Frequency',\n",
      "       'Avg Packet Len Ac Magnitude', 'Avg Packet Len Ac Frequency',\n",
      "       'Source IP Count Ac Magnitude', 'Source IP Count Ac Frequency'],\n",
      "      dtype='object')\n",
      "processing a\n",
      "processing b\n",
      "processing c\n",
      "processing d\n"
     ]
    }
   ],
   "source": [
    "components_a = pd.read_csv(\"/home/appuser/data/train/SCLDDoS2024_SetA_components.csv\")\n",
    "events_a = pd.read_csv(\"/home/appuser/data/train/SCLDDoS2024_SetA_events.csv\")\n",
    "\n",
    "components_b = pd.read_csv(\"/home/appuser/data/train/SCLDDoS2024_SetB_components.csv\")\n",
    "events_b = pd.read_csv(\"/home/appuser/data/train/SCLDDoS2024_SetB_events.csv\")\n",
    "\n",
    "components_c = pd.read_csv(\"/home/appuser/data/test/SCLDDoS2024_SetC_components.csv\")\n",
    "events_c = pd.read_csv(\"/home/appuser/data/test/SCLDDoS2024_SetC_events.csv\")\n",
    "\n",
    "components_d = pd.read_csv(\"/home/appuser/data/gen/SCLDDoS2024_SetD_components.csv\")\n",
    "events_d = pd.read_csv(\"/home/appuser/data/gen/SCLDDoS2024_SetD_events.csv\")\n",
    "\n",
    "events_a = process_components(events_a, components_a)\n",
    "\n",
    "events_b = process_components(events_b, components_b)\n",
    "\n",
    "events_c = process_components(events_c, components_c)\n",
    "\n",
    "events_d = process_components(events_d, components_d)\n",
    "\n",
    "print(\"processing a\")\n",
    "events_extended_a = add_time_features(events_a)\n",
    "events_extended_a.to_csv(\"/home/appuser/data/train/SCLDDoS2024_SetA_events_extended.csv\", index=False)\n",
    "\n",
    "print(\"processing b\")\n",
    "events_extended_b = add_time_features(events_b)\n",
    "events_extended_b.to_csv(\"/home/appuser/data/train/SCLDDoS2024_SetB_events_extended.csv\", index=False)\n",
    "\n",
    "print(\"processing c\")\n",
    "events_extended_c = add_time_features(events_c)\n",
    "events_extended_c.to_csv(\"/home/appuser/data/test/SCLDDoS2024_SetC_events_extended.csv\", index=False)\n",
    "\n",
    "print(\"processing d\")\n",
    "events_extended_d = add_time_features(events_d)\n",
    "events_extended_d.to_csv(\"/home/appuser/data/gen/SCLDDoS2024_SetD_events_extended.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e7f6d1",
   "metadata": {},
   "source": [
    "# Further Feature Engineering with Inferred Attack Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2dbb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding The Inferred Attack Code columns and saving it, so we don't need to wait through it mutliple times\n",
    "CAT_TO_NUM_LABELS = {\n",
    "    \"Normal traffic\": 0,\n",
    "    \"Suspicious traffic\": 1,\n",
    "    \"DDoS attack\": 2,\n",
    "}\n",
    "\n",
    "component_columns = [\n",
    "    \"Attack ID\", \"Detect count\", \"Card\", \"Victim IP\", \"Port number\",\n",
    "    \"Attack code\", \"Significant flag\", \"Packet speed\", \"Data speed\", \"Avg packet len\",\n",
    "    \"Source IP count\", \"Time\"\n",
    "]\n",
    "\n",
    "event_columns = [\n",
    "    \"Attack ID\", \"Card\", \"Victim IP\", \"Port number\", \"Attack code\", \n",
    "    \"Detect count\", \"Significant flag\", \"Packet speed\", \"Data speed\", \n",
    "    \"Avg packet len\", \"Avg source IP count\", \"Start time\", \"End time\", \n",
    "    \"Whitelist flag\", \"Type\"\n",
    "]\n",
    "\n",
    "class DDoSDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.train_data_paths = [f'/home/appuser/data/train/SCLDDoS2024_SetA_events_extended.csv',\n",
    "                                 f'/home/appuser/data/train/SCLDDoS2024_SetB_events_extended.csv']\n",
    "        self.test_data_paths = [f'/home/appuser/data/test/SCLDDoS2024_SetC_events_extended.csv']     \n",
    "        \n",
    "        self.gen_data_paths = [f'/home/appuser/data/gen/SCLDDoS2024_SetD_events_extended.csv',]\n",
    "        \n",
    "        self.split = split   \n",
    "        \n",
    "        if split == 'train':\n",
    "            self.load_data(self.train_data_paths, apply_smote=False)\n",
    "        elif split == 'test':\n",
    "            self.load_data(self.test_data_paths, apply_smote=False)\n",
    "        elif split == 'gen':\n",
    "            self.load_data(self.gen_data_paths, apply_smote=False)\n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid split. Use 'train' or 'test'\")\n",
    "            \n",
    "    \n",
    "    def get_ports(self):\n",
    "        return self.ddos_ports\n",
    "    \n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.features.numpy(), self.lables.numpy()\n",
    "    \n",
    "    def engineer_features_from_components(self, df_components):\n",
    "\n",
    "        grouped = df_components.groupby('Attack ID')\n",
    "\n",
    "        features = pd.DataFrame()\n",
    "\n",
    "        features['Unique Ports'] = grouped['Port number'].nunique()\n",
    "        features['Unique Victim IPs'] = grouped['Victim IP'].nunique()\n",
    "\n",
    "        return features.reset_index()\n",
    "    \n",
    "    # Function to infer attack codes for a full attack group\n",
    "    def infer_attack_code_row(self, row):\n",
    "        codes = set()\n",
    "\n",
    "        # CHARGEN:\n",
    "        if row[\"Packet speed\"] > 500000 and row[\"Data speed\"] > 400 and row[\"Port number\"] == 443:\n",
    "            codes.add(\"CHARGEN\")\n",
    "            \n",
    "        # CLDAP:\n",
    "        if row[\"Detect count\"] >= 10 and row[\"Data speed\"] > 400 and row[\"Port number\"] in [389, 53,80,443,0]:\n",
    "            codes.add(\"CLDAP\")\n",
    "            \n",
    "        # CoAP: I don't see any indicators for this\n",
    "        \n",
    "        # # DNS: notghing specific but we can use the port number\n",
    "        # if row[\"Port number\"] in [53, 443] and row[\"Data speed\"] < 30:\n",
    "        #     codes.add(\"DNS\")\n",
    "            \n",
    "        # Generic UDP:\n",
    "        if row[\"Data speed\"] < 20 and row[\"Port number\"] in [0,80, 56, 5656, 4500]:\n",
    "            codes.add(\"Generic UDP\")  \n",
    "            \n",
    "        # IPV4 fragmentation:\n",
    "        if row[\"Packet speed\"] > 1000000 and row[\"Data speed\"] > 1000 and row[\"Port number\"] in [0,80,443]:\n",
    "            codes.add(\"IPV4 fragmentation\")\n",
    "            \n",
    "        # NTP: I don't see anything\n",
    "        \n",
    "        # RDP: same\n",
    "        \n",
    "        # RPC: same\n",
    "        \n",
    "        # SNMP: same\n",
    "        \n",
    "        # SSDP: same\n",
    "        \n",
    "        # SYN Attack:\n",
    "        if row[\"Data speed\"] <= 10 and row[\"Avg packet len\"] <= 10 and row[\"Port number\"] in [80,11,22, 443, 0]:\n",
    "            codes.add(\"SYN Attack\")\n",
    "            \n",
    "        # Sentinel:\n",
    "        if row[\"Packet speed\"] < 30000 and row[\"Data speed\"] < 10 and row[\"Port number\"] == 0:\n",
    "            codes.add(\"Sentinel\")\n",
    "            \n",
    "        # TCP Anomaly:\n",
    "        if row[\"Avg packet len\"] == 0:\n",
    "            codes.add(\"TCP Anomaly\")\n",
    "\n",
    "        return \"; \".join(sorted(codes)) if codes else \"Unknown\"\n",
    "        \n",
    "    # preload the data as it makes the training much faster (and it easily fits in memory)\n",
    "    def load_data(self, data_paths, apply_smote=False, undersample=False, sample_factor=4, add_features=True):\n",
    "        data = []\n",
    "        component_data = []\n",
    "        \n",
    "        for path in data_paths:\n",
    "            event_df = pd.read_csv(path).fillna(0)\n",
    "            data.append(event_df)\n",
    "\n",
    "            # Attempt to load corresponding component file\n",
    "            comp_path = path.replace('_events_extended.csv', '_components.csv')\n",
    "            ref_event_path = path.replace('_events_extended.csv', '_events.csv')\n",
    "            if os.path.exists(comp_path) and add_features:\n",
    "                # Load event data\n",
    "                ref_ev_df = pd.read_csv(ref_event_path).fillna(0)\n",
    "                ref_ev_df.columns = event_columns\n",
    "\n",
    "                # Filter out invalid 'Attack ID's based on 'End time'\n",
    "                ref_ev_df2 = ref_ev_df[ref_ev_df['End time'].astype(str) != '0']\n",
    "                invalid_attack_ids = ref_ev_df[ref_ev_df['End time'].astype(str) == '0']['Attack ID'].unique()\n",
    "\n",
    "                # Filter the event data by removing rows with invalid 'Attack ID's\n",
    "                valid_attack_ids = ref_ev_df2['Attack ID'].unique()  # Attack IDs present in valid events\n",
    "\n",
    "                # Load component data\n",
    "                component_df = pd.read_csv(comp_path).fillna(0)\n",
    "                component_df.columns = component_columns\n",
    "\n",
    "                # Remove invalid attack IDs from component data\n",
    "                component_df = component_df[~component_df['Attack ID'].isin(invalid_attack_ids)]\n",
    "                \n",
    "                # Now filter component data to only include 'Attack ID's present in valid events\n",
    "                component_df = component_df[component_df['Attack ID'].isin(valid_attack_ids)]\n",
    "\n",
    "                # Append the filtered component data\n",
    "                component_data.append(component_df)\n",
    "            else:\n",
    "                print(f\"Component file not found: {comp_path}\")\n",
    "        \n",
    "        df = pd.concat(data, ignore_index=True)\n",
    "        \n",
    "        if component_data:\n",
    "            # df_components = pd.concat(component_data, ignore_index=True)\n",
    "            \n",
    "            # attack_id_to_code = (\n",
    "            #     df_components.groupby(\"Attack ID\")\n",
    "            #     .apply(self.infer_attack_code_group)\n",
    "            #     .rename(\"Inferred Attack Code\")\n",
    "            # )\n",
    "            \n",
    "            # Merge back the inferred attack code to all component rows\n",
    "            #df_components = df_components.merge(attack_id_to_code, on=\"Attack ID\")\n",
    "            \n",
    "            #comp_features = self.engineer_features_from_components(df_components)\n",
    "            df[\"Inferred Attack Code\"] = df.apply(self.infer_attack_code_row, axis=1)\n",
    "            # df = pd.concat([df, attack_id_to_code], axis=1)\n",
    "            # df = df.drop(columns=['Attack ID'])\n",
    "            cols = list(df.columns)\n",
    "            cols[-2], cols[-1] = cols[-1], cols[-2]\n",
    "            df = df[cols]\n",
    "            df = df.dropna(how='all')\n",
    "            \n",
    "        # Save the dataframes to a CSV file\n",
    "        if self.split == 'train':\n",
    "            df.to_csv('/home/appuser/data/train/A_B_inferred_attack_code.csv', index=False)\n",
    "        elif self.split == 'test':\n",
    "            df.to_csv('/home/appuser/data/test/C_inferred_attack_code.csv', index=False)\n",
    "        elif self.split == 'gen':\n",
    "            df.to_csv('/home/appuser/data/gen/D_inferred_attack_code.csv', index=False)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8692723",
   "metadata": {},
   "source": [
    "### Example usage    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e881c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = DDoSDataset('train')\n",
    "test = DDoSDataset('test')\n",
    "gen = DDoSDataset('gen')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
