{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabM\n",
    "\n",
    "This is a standalone usage example for the TabM project.\n",
    "The easiest way to run it is [Pixi](https://pixi.sh/latest/#installation):\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/yandex-research/tabm\n",
    "cd tabm\n",
    "\n",
    "# With GPU:\n",
    "pixi run -e cuda jupyter-lab example.ipynb\n",
    "\n",
    "# Without GPU:\n",
    "pixi run jupyter-lab example.ipynb\n",
    "```\n",
    "\n",
    "For the full overview of the project, and for non-Pixi environment setups, see README in the repository:\n",
    "https://github.com/yandex-research/tabm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from typing import Literal, NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import rtdl_num_embeddings  # https://github.com/yandex-research/rtdl-num-embeddings\n",
    "import scipy.special\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch import Tensor\n",
    "from tqdm.std import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "from tabm_reference import Model, make_parameter_groups\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed + 1)\n",
    "torch.manual_seed(seed + 2)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_TO_NUM_LABELS = {\n",
    "    \"Normal traffic\": 0,\n",
    "    \"Suspicious traffic\": 1,\n",
    "    \"DDoS attack\": 2,\n",
    "}\n",
    "\n",
    "class DDoSDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.train_data_paths = [f'/home/appuser/data/train/SCLDDoS2024_SetA_events_extended.csv',\n",
    "                                 f'/home/appuser/data/train/SCLDDoS2024_SetB_events_extended.csv']\n",
    "        self.test_data_paths = [f'/home/appuser/data/test/SCLDDoS2024_SetC_events_extended.csv']     \n",
    "        \n",
    "        self.split = split   \n",
    "        \n",
    "        if split == 'train':\n",
    "            self.features, self.lables = self.load_data(self.train_data_paths, apply_smote=False)\n",
    "        elif split == 'test':\n",
    "            self.features, self.lables = self.load_data(self.test_data_paths, apply_smote=False)\n",
    "        else:\n",
    "            print(\"Invalid split. Use 'train' or 'test'\")\n",
    "            \n",
    "    \n",
    "    def get_ports(self):\n",
    "        return self.ddos_ports\n",
    "    \n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.features.numpy(), self.lables.numpy()\n",
    "        \n",
    "        \n",
    "    # preload the data as it makes the training much faster (and it easily fits in memory)\n",
    "    def load_data(self, data_paths, apply_smote=False, undersample=False, sample_factor=4):\n",
    "        data = []\n",
    "        for path in data_paths:\n",
    "            data.append(pd.read_csv(path).fillna(0))  # Read and fill NaNs with 0s\n",
    "            \n",
    "        df = pd.concat(data, ignore_index=True)  # Combine all dataframes\n",
    "        \n",
    "        self.ddos_ports = df[df['Type'] == \"DDoS attack\"][\"Port number\"].unique()\n",
    "        \n",
    "        feature_columns = df.columns[:19]  # All except the last column\n",
    "        #feature_columns = df.columns[:-1]\n",
    "        label_column = df.columns[-1]  # The last column\n",
    "        \n",
    "        # Convert categorical labels to numeric using the dictionary\n",
    "        df[label_column] = df[label_column].map(CAT_TO_NUM_LABELS)\n",
    "        \n",
    "        # Check for missing or unknown labels\n",
    "        if df[label_column].isna().any():\n",
    "            print(df[label_column].isna().sum(), \"missing labels\")\n",
    "\n",
    "        features = df[feature_columns].values\n",
    "        labels = df[label_column].values\n",
    "        \n",
    "        # Normalize the features\n",
    "        #features = self.normalize(features)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        features = torch.tensor(features, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.long)  # Classification requires long dtype\n",
    "        \n",
    "        \n",
    "        \n",
    "        if undersample and self.split == 'train':\n",
    "            # Undersample the majority class (label=0)\n",
    "            class_0_indices = np.where(labels.cpu().numpy() == 0)[0]\n",
    "            class_1_indices = np.where(labels.cpu().numpy() == 1)[0]\n",
    "            class_2_indices = np.where(labels.cpu().numpy() == 2)[0]\n",
    "\n",
    "            # Randomly undersample the majority class\n",
    "            num_class_0_samples = sample_factor*(len(class_1_indices) + len(class_2_indices))  # Same number as the minority class\n",
    "            class_0_indices_undersampled = np.random.choice(class_0_indices, num_class_0_samples, replace=False)\n",
    "\n",
    "            # Concatenate indices of class 1, 2, and undersampled class 0\n",
    "            undersampled_indices = np.concatenate([class_0_indices_undersampled, class_1_indices, class_2_indices])\n",
    "\n",
    "            # Subset the dataset to include only the sampled indices\n",
    "            features = features[undersampled_indices]\n",
    "            labels = labels[undersampled_indices]\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.lables[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Dataset.\n",
    "TaskType = Literal['regression', 'binclass', 'multiclass']\n",
    "\n",
    "# Regression.\n",
    "task_type: TaskType = 'multiclass'\n",
    "n_classes = None\n",
    "dataset = sklearn.datasets.fetch_california_housing()\n",
    "X_cont: np.ndarray = dataset['data']\n",
    "Y: np.ndarray = dataset['target']\n",
    "\n",
    "dataset = DDoSDataset(split='train')\n",
    "X_cont = dataset.features.numpy()\n",
    "Y = dataset.lables.numpy()\n",
    "\n",
    "\n",
    "# Classification.\n",
    "n_classes = 3\n",
    "assert n_classes >= 2\n",
    "task_type: TaskType = 'binclass' if n_classes == 2 else 'multiclass'\n",
    "# X_cont, Y = sklearn.datasets.make_classification(\n",
    "#     n_samples=20000,\n",
    "#     n_features=8,\n",
    "#     n_classes=n_classes,\n",
    "#     n_informative=3,\n",
    "#     n_redundant=2,\n",
    "# )\n",
    "\n",
    "#task_is_regression = task_type == 'regression'\n",
    "\n",
    "# >>> Continuous features.\n",
    "X_cont: np.ndarray = X_cont.astype(np.float32)\n",
    "n_cont_features = X_cont.shape[1]\n",
    "\n",
    "# # >>> Categorical features.\n",
    "# # NOTE: the above datasets do not have categorical features, however,\n",
    "# # for the demonstration purposes, it is possible to generate them.\n",
    "# cat_cardinalities = [\n",
    "#     # NOTE: uncomment the two lines below to add two categorical features.\n",
    "#     # 4,  # Allowed values: [0, 1, 2, 3].\n",
    "#     # 7,  # Allowed values: [0, 1, 2, 3, 4, 5, 6].\n",
    "# ]\n",
    "# X_cat = (\n",
    "#     np.column_stack(\n",
    "#         [np.random.randint(0, c, (len(X_cont),)) for c in cat_cardinalities]\n",
    "#     )\n",
    "#     if cat_cardinalities\n",
    "#     else None\n",
    "# )\n",
    "\n",
    "# >>> Labels.\n",
    "if task_type == 'regression':\n",
    "    Y = Y.astype(np.float32)\n",
    "else:\n",
    "    assert n_classes is not None\n",
    "    Y = Y.astype(np.int64)\n",
    "    assert set(Y.tolist()) == set(\n",
    "        range(n_classes)\n",
    "    ), 'Classification labels must form the range [0, 1, ..., n_classes - 1]'\n",
    "\n",
    "# >>> Split the dataset.\n",
    "all_idx = np.arange(len(Y))\n",
    "train_idx, val_idx = sklearn.model_selection.train_test_split(\n",
    "    all_idx, train_size=0.8\n",
    ")\n",
    "\n",
    "data_numpy = {\n",
    "    'train': {'x_cont': X_cont[train_idx], 'y': Y[train_idx]},\n",
    "    'val': {'x_cont': X_cont[val_idx], 'y': Y[val_idx]},\n",
    "}\n",
    "\n",
    "s = 0\n",
    "# if X_cat is not None:\n",
    "#     data_numpy['train']['x_cat'] = X_cat[train_idx]\n",
    "#     data_numpy['val']['x_cat'] = X_cat[val_idx]\n",
    "#     data_numpy['test']['x_cat'] = X_cat[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Dataset C as the testing class\n",
    "dataset = DDoSDataset(split='test')\n",
    "X_cont = dataset.features.numpy()\n",
    "Y = dataset.lables.numpy()\n",
    "\n",
    "\n",
    "# Classification.\n",
    "n_classes = 3\n",
    "assert n_classes >= 2\n",
    "task_type: TaskType = 'binclass' if n_classes == 2 else 'multiclass'\n",
    "# X_cont, Y = sklearn.datasets.make_classification(\n",
    "#     n_samples=20000,\n",
    "#     n_features=8,\n",
    "#     n_classes=n_classes,\n",
    "#     n_informative=3,\n",
    "#     n_redundant=2,\n",
    "# )\n",
    "\n",
    "#task_is_regression = task_type == 'regression'\n",
    "\n",
    "# >>> Continuous features.\n",
    "X_cont: np.ndarray = X_cont.astype(np.float32)\n",
    "n_cont_features = X_cont.shape[1]\n",
    "\n",
    "# >>> Labels.\n",
    "if task_type == 'regression':\n",
    "    Y = Y.astype(np.float32)\n",
    "else:\n",
    "    assert n_classes is not None\n",
    "    Y = Y.astype(np.int64)\n",
    "    assert set(Y.tolist()) == set(\n",
    "        range(n_classes)\n",
    "    ), 'Classification labels must form the range [0, 1, ..., n_classes - 1]'\n",
    "    \n",
    "    \n",
    "data_numpy['test'] = {'x_cont': X_cont, 'y': Y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature preprocessing.\n",
    "# NOTE\n",
    "# The choice between preprocessing strategies depends on a task and a model.\n",
    "\n",
    "# Simple preprocessing strategy.\n",
    "# preprocessing = sklearn.preprocessing.StandardScaler().fit(\n",
    "#     data_numpy['train']['x_cont']\n",
    "# )\n",
    "\n",
    "# Advanced preprocessing strategy.\n",
    "# The noise is added to improve the output of QuantileTransformer in some cases.\n",
    "X_cont_train_numpy = data_numpy['train']['x_cont']\n",
    "noise = (\n",
    "    np.random.default_rng(0)\n",
    "    .normal(0.0, 1e-5, X_cont_train_numpy.shape)\n",
    "    .astype(X_cont_train_numpy.dtype)\n",
    ")\n",
    "preprocessing = sklearn.preprocessing.QuantileTransformer(\n",
    "    n_quantiles=max(min(len(train_idx) // 30, 1000), 10),\n",
    "    output_distribution='normal',\n",
    "    subsample=10**9,\n",
    ").fit(X_cont_train_numpy + noise)\n",
    "del X_cont_train_numpy\n",
    "\n",
    "# Apply the preprocessing.\n",
    "for part in data_numpy:\n",
    "    data_numpy[part]['x_cont'] = preprocessing.transform(data_numpy[part]['x_cont'])\n",
    "\n",
    "\n",
    "# Label preprocessing.\n",
    "class RegressionLabelStats(NamedTuple):\n",
    "    mean: float\n",
    "    std: float\n",
    "\n",
    "\n",
    "Y_train = data_numpy['train']['y'].copy()\n",
    "if task_type == 'regression':\n",
    "    # For regression tasks, it is highly recommended to standardize the training labels.\n",
    "    regression_label_stats = RegressionLabelStats(\n",
    "        Y_train.mean().item(), Y_train.std().item()\n",
    "    )\n",
    "    Y_train = (Y_train - regression_label_stats.mean) / regression_label_stats.std\n",
    "else:\n",
    "    regression_label_stats = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PyTorch settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:        CUDA\n",
      "AMP:           False (dtype: torch.bfloat16)\n",
      "torch.compile: False\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert data to tensors\n",
    "data = {\n",
    "    part: {k: torch.as_tensor(v, device=device) for k, v in data_numpy[part].items()}\n",
    "    for part in data_numpy\n",
    "}\n",
    "Y_train = torch.as_tensor(Y_train, device=device)\n",
    "if task_type == 'regression':\n",
    "    for part in data:\n",
    "        data[part]['y'] = data[part]['y'].float()\n",
    "    Y_train = Y_train.float()\n",
    "\n",
    "# Automatic mixed precision (AMP)\n",
    "# torch.float16 is implemented for completeness,\n",
    "# but it was not tested in the project,\n",
    "# so torch.bfloat16 is used by default.\n",
    "amp_dtype = (\n",
    "    torch.bfloat16\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else torch.float16\n",
    "    if torch.cuda.is_available()\n",
    "    else None\n",
    ")\n",
    "# Changing False to True will result in faster training on compatible hardware.\n",
    "amp_enabled = False and amp_dtype is not None\n",
    "grad_scaler = torch.cuda.amp.GradScaler() if amp_dtype is torch.float16 else None  # type: ignore\n",
    "\n",
    "# torch.compile\n",
    "compile_model = False\n",
    "\n",
    "# fmt: off\n",
    "print(\n",
    "    f'Device:        {device.type.upper()}'\n",
    "    f'\\nAMP:           {amp_enabled} (dtype: {amp_dtype})'\n",
    "    f'\\ntorch.compile: {compile_model}'\n",
    ")\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one of the two configurations below.\n",
    "\n",
    "# TabM\n",
    "arch_type = 'tabm'\n",
    "bins = None\n",
    "\n",
    "# TabM-mini with the piecewise-linear embeddings.\n",
    "# arch_type = 'tabm-mini'\n",
    "# bins = rtdl_num_embeddings.compute_bins(data['train']['x_cont'])\n",
    "\n",
    "# arch_type = 'tabm-packed'\n",
    "# bins = rtdl_num_embeddings.compute_bins(data['train']['x_cont'])\n",
    "\n",
    "# d_block: 512\n",
    "# n_blocks: 3\n",
    "\n",
    "model = Model(\n",
    "    n_num_features=n_cont_features,\n",
    "    cat_cardinalities=[],\n",
    "    n_classes=n_classes,\n",
    "    backbone={\n",
    "        'type': 'MLP',\n",
    "        'n_blocks': 3 if bins is None else 2,\n",
    "        'd_block': 256,\n",
    "        'dropout': 0.2,\n",
    "        'n_blocks': 5\n",
    "    },\n",
    "    bins=bins,\n",
    "    num_embeddings=(\n",
    "        None\n",
    "        if bins is None\n",
    "        else {\n",
    "            'type': 'PiecewiseLinearEmbeddings',\n",
    "            'd_embedding': 32,\n",
    "            'activation': False,\n",
    "            'version': 'B',\n",
    "        }\n",
    "    ),\n",
    "    arch_type=arch_type,\n",
    "    k=48,\n",
    "    share_training_batches=True,\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(make_parameter_groups(model), lr=1e-3, weight_decay=3e-4)\n",
    "\n",
    "if compile_model:\n",
    "    # NOTE\n",
    "    # `torch.compile` is intentionally called without the `mode` argument\n",
    "    # (mode=\"reduce-overhead\" caused issues during training with torch==2.0.1).\n",
    "    model = torch.compile(model)\n",
    "    evaluation_mode = torch.no_grad\n",
    "else:\n",
    "    evaluation_mode = torch.inference_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score before training: 0.0153\n"
     ]
    }
   ],
   "source": [
    "@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
    "def apply_model(part: str, idx: Tensor) -> Tensor:\n",
    "    return (\n",
    "        model(\n",
    "            data[part]['x_cont'][idx],\n",
    "            data[part]['x_cat'][idx] if 'x_cat' in data[part] else None,\n",
    "        )\n",
    "        .squeeze(-1)  # Remove the last dimension for regression tasks.\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "\n",
    "base_loss_fn = F.mse_loss if task_type == 'regression' else F.cross_entropy\n",
    "weight = torch.tensor([1.0, 1.0, 3.0], device=device)\n",
    "\n",
    "\n",
    "def loss_fn(y_pred: Tensor, y_true: Tensor) -> Tensor:\n",
    "    # TabM produces k predictions. Each of them must be trained separately.\n",
    "    # (regression)     y_pred.shape == (batch_size, k)\n",
    "    # (classification) y_pred.shape == (batch_size, k, n_classes)\n",
    "    k = y_pred.shape[-1 if task_type == 'regression' else -2]\n",
    "    return base_loss_fn(\n",
    "        y_pred.flatten(0, 1),\n",
    "        y_true.repeat_interleave(k) if model.share_training_batches else y_true,\n",
    "        weight=weight,\n",
    "    )\n",
    "\n",
    "\n",
    "@evaluation_mode()\n",
    "def evaluate(part: str) -> float:\n",
    "    model.eval()\n",
    "\n",
    "    # When using torch.compile, you may need to reduce the evaluation batch size.\n",
    "    eval_batch_size = 8096\n",
    "    y_pred: np.ndarray = (\n",
    "        torch.cat(\n",
    "            [\n",
    "                apply_model(part, idx)\n",
    "                for idx in torch.arange(len(data[part]['y']), device=device).split(\n",
    "                    eval_batch_size\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    if task_type == 'regression':\n",
    "        # Transform the predictions back to the original label space.\n",
    "        assert regression_label_stats is not None\n",
    "        y_pred = y_pred * regression_label_stats.std + regression_label_stats.mean\n",
    "\n",
    "    # Compute the mean of the k predictions.\n",
    "    if task_type != 'regression':\n",
    "        # For classification, the mean must be computed in the probabily space.\n",
    "        y_pred = scipy.special.softmax(y_pred, axis=-1)\n",
    "    y_pred = y_pred.mean(1)\n",
    "\n",
    "    y_true = data[part]['y'].cpu().numpy()\n",
    "    score = (\n",
    "        -(sklearn.metrics.mean_squared_error(y_true, y_pred) ** 0.5)\n",
    "        if task_type == 'regression'\n",
    "        else sklearn.metrics.f1_score(y_true, y_pred.argmax(1), average='macro')\n",
    "    )\n",
    "    return float(score)  # The higher -- the better.\n",
    "\n",
    "\n",
    "print(f'Test score before training: {evaluate(\"test\"):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.7652 (test) 0.7199\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.7554 (test) 0.7836\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8198 (test) 0.7800\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 119.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8349 (test) 0.7723\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8302 (test) 0.7797\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 118.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8235 (test) 0.7903\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8212 (test) 0.8021\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8373 (test) 0.7889\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8407 (test) 0.7883\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8351 (test) 0.8080\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8355 (test) 0.8004\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8449 (test) 0.7764\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8394 (test) 0.7903\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8363 (test) 0.7961\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8412 (test) 0.8304\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8402 (test) 0.8203\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 118.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8471 (test) 0.8020\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 119.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8439 (test) 0.8121\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8411 (test) 0.8225\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8474 (test) 0.8266\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 119.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8389 (test) 0.7732\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8465 (test) 0.8435\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 119.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8425 (test) 0.7917\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8481 (test) 0.8238\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 118.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8456 (test) 0.8211\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8493 (test) 0.8361\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8365 (test) 0.8260\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8491 (test) 0.8426\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 118.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8477 (test) 0.8408\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 118.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8535 (test) 0.8320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8514 (test) 0.8251\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8521 (test) 0.8328\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 119.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8509 (test) 0.8359\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 121.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8468 (test) 0.8525\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 121.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8479 (test) 0.7500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8470 (test) 0.7696\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8558 (test) 0.8401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8482 (test) 0.8548\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8502 (test) 0.8191\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 120.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8468 (test) 0.7993\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:06<00:00, 119.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8453 (test) 0.7851\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 476/828 [00:03<00:02, 121.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grad_scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 40\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     grad_scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:177\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    174\u001b[0m     amsgrad \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    175\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 177\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m     adamw(\n\u001b[1;32m    189\u001b[0m         params_with_grad,\n\u001b[1;32m    190\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/adamw.py:104\u001b[0m, in \u001b[0;36mAdamW._init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    102\u001b[0m has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     has_complex \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_complex(p)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For demonstration purposes (fast training and bad performance),\n",
    "# one can set smaller values:\n",
    "# n_epochs = 20\n",
    "# patience = 2\n",
    "n_epochs = 200\n",
    "\n",
    "train_size = len(train_idx)\n",
    "batch_size = 256\n",
    "epoch_size = math.ceil(train_size / batch_size)\n",
    "best = {\n",
    "    'val': -math.inf,\n",
    "    'test': -math.inf,\n",
    "    'epoch': -1,\n",
    "}\n",
    "# Early stopping: the training stops when\n",
    "# there are more than `patience` consequtive bad updates.\n",
    "patience = 200\n",
    "remaining_patience = patience\n",
    "\n",
    "\n",
    "\n",
    "print('-' * 88 + '\\n')\n",
    "for epoch in range(n_epochs):\n",
    "    batches = (\n",
    "        torch.randperm(train_size, device=device).split(batch_size)\n",
    "        if model.share_training_batches\n",
    "        else [\n",
    "            x.transpose(0, 1).flatten()\n",
    "            for x in torch.rand((model.k, train_size), device=device)\n",
    "            .argsort(dim=1)\n",
    "            .split(batch_size, dim=1)\n",
    "        ]\n",
    "    )\n",
    "    for batch_idx in tqdm(batches, desc=f'Epoch {epoch}'):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(apply_model('train', batch_idx), Y_train[batch_idx])\n",
    "        if grad_scaler is None:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            grad_scaler.scale(loss).backward()  # type: ignore\n",
    "            grad_scaler.step(optimizer)\n",
    "            grad_scaler.update()\n",
    "\n",
    "    val_score = evaluate('val')\n",
    "    test_score = evaluate('test')\n",
    "    print(f'(val) {val_score:.4f} (test) {test_score:.4f}')\n",
    "\n",
    "    if test_score > best['test']:\n",
    "        print('ðŸŒ¸ New best epoch! ðŸŒ¸')\n",
    "        best = {'val': val_score, 'test': test_score, 'epoch': epoch}\n",
    "        remaining_patience = patience\n",
    "        torch.save(model.state_dict(), 'tabm_no_frequ_features_extended_more_weight.pth')\n",
    "    else:\n",
    "        remaining_patience -= 1\n",
    "\n",
    "    if remaining_patience < 0:\n",
    "        break\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "print('\\n\\nResult:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "arch_type = 'tabm'\n",
    "bins = None\n",
    "\n",
    "arch_type = 'tabm-mini'\n",
    "bins = rtdl_num_embeddings.compute_bins(data['train']['x_cont'])\n",
    "\n",
    "model = Model(\n",
    "    n_num_features=n_cont_features,\n",
    "    cat_cardinalities=[],\n",
    "    n_classes=n_classes,\n",
    "    backbone={\n",
    "        'type': 'MLP',\n",
    "        'n_blocks': 3 if bins is None else 2,\n",
    "        'd_block': 512,\n",
    "        'dropout': 0.1,\n",
    "    },\n",
    "    bins=bins,\n",
    "    num_embeddings=(\n",
    "        None\n",
    "        if bins is None\n",
    "        else {\n",
    "            'type': 'PiecewiseLinearEmbeddings',\n",
    "            'd_embedding': 16,\n",
    "            'activation': False,\n",
    "            'version': 'B',\n",
    "        }\n",
    "    ),\n",
    "    arch_type=arch_type,\n",
    "    k=32,\n",
    "    share_training_batches=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
    "def apply_model(part: str, idx: Tensor) -> Tensor:\n",
    "    return (\n",
    "        model(\n",
    "            data[part]['x_cont'][idx],\n",
    "            data[part]['x_cat'][idx] if 'x_cat' in data[part] else None,\n",
    "        )\n",
    "        .squeeze(-1)  # Remove the last dimension for regression tasks.\n",
    "        .float()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10834/10834 [00:06<00:00, 1656.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9917\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score        support\n",
      "0              0.995075  0.996727  0.995901  125892.000000\n",
      "1              0.887179  0.963630  0.923826    3052.000000\n",
      "2              0.861063  0.475829  0.612943    1055.000000\n",
      "accuracy       0.991723  0.991723  0.991723       0.991723\n",
      "macro avg      0.914439  0.812062  0.844223  129999.000000\n",
      "weighted avg   0.991455  0.991723  0.991101  129999.000000\n",
      "\n",
      "F1 (Micro): 0.9917\n",
      "F1 (Macro): 0.8442\n",
      "\n",
      "Class-wise Accuracy (Recall):\n",
      " 0    0.996727\n",
      "1    0.963630\n",
      "2    0.475829\n",
      "Name: recall, dtype: float64\n",
      "\n",
      "Confusion Matrix:\n",
      " [[125480    350     62]\n",
      " [    92   2941     19]\n",
      " [   529     24    502]]\n"
     ]
    }
   ],
   "source": [
    "# Inference on the test dataset\n",
    "model.load_state_dict(torch.load('m_ft_transformer_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "part = \"test\"\n",
    "\n",
    "eval_batch_size = 12\n",
    "# y_pred: np.ndarray = (\n",
    "#     torch.cat(\n",
    "#         [\n",
    "#             apply_model(part, idx).cpu()\n",
    "#             for idx in torch.arange(len(data[part]['y']), device=device).split(\n",
    "#                 eval_batch_size\n",
    "#             )\n",
    "#         ]\n",
    "#     )\n",
    "#     .numpy()\n",
    "# )\n",
    "\n",
    "y_pred_list = []\n",
    "for idx in tqdm(torch.arange(len(data[part]['y']), device=device).split(eval_batch_size)):\n",
    "    with torch.no_grad():\n",
    "        preds = apply_model(part, idx).cpu()\n",
    "        probs = scipy.special.softmax(preds.numpy(), axis=-1)\n",
    "        averaged = probs.mean(1)  # shape: [B, C]\n",
    "        preds_class = np.argmax(averaged, axis=1)\n",
    "        y_pred_list.append(preds_class)\n",
    "\n",
    "y_pred = np.concatenate(y_pred_list)\n",
    "\n",
    "y_test = data[part]['y'].cpu().numpy()\n",
    "\n",
    "# Overall accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report (includes precision, recall, F1 per class + macro/micro)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "print(\"\\nClassification Report:\\n\", report_df)\n",
    "\n",
    "# F1 scores\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"\\nF1 (Micro): {f1_micro:.4f}\")\n",
    "print(f\"F1 (Macro): {f1_macro:.4f}\")\n",
    "\n",
    "# Class-wise accuracy (same as recall per class)\n",
    "class_wise_accuracy = report_df.loc[[str(i) for i in np.unique(y_test)], \"recall\"]\n",
    "print(\"\\nClass-wise Accuracy (Recall):\\n\", class_wise_accuracy)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
