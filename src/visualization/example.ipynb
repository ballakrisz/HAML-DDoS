{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabM\n",
    "\n",
    "This is a standalone usage example for the TabM project.\n",
    "The easiest way to run it is [Pixi](https://pixi.sh/latest/#installation):\n",
    "\n",
    "```shell\n",
    "git clone https://github.com/yandex-research/tabm\n",
    "cd tabm\n",
    "\n",
    "# With GPU:\n",
    "pixi run -e cuda jupyter-lab example.ipynb\n",
    "\n",
    "# Without GPU:\n",
    "pixi run jupyter-lab example.ipynb\n",
    "```\n",
    "\n",
    "For the full overview of the project, and for non-Pixi environment setups, see README in the repository:\n",
    "https://github.com/yandex-research/tabm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E402\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from typing import Literal, NamedTuple\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import rtdl_num_embeddings  # https://github.com/yandex-research/rtdl-num-embeddings\n",
    "import scipy.special\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from torch import Tensor\n",
    "from tqdm.std import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from collections import Counter\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "from tabm_reference import Model, make_parameter_groups\n",
    "\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed + 1)\n",
    "torch.manual_seed(seed + 2)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding The Inferred Attack Code columns and saving it, so we don't need to wait through it mutliple times\n",
    "CAT_TO_NUM_LABELS = {\n",
    "    \"Normal traffic\": 0,\n",
    "    \"Suspicious traffic\": 1,\n",
    "    \"DDoS attack\": 2,\n",
    "}\n",
    "\n",
    "component_columns = [\n",
    "    \"Attack ID\", \"Detect count\", \"Card\", \"Victim IP\", \"Port number\",\n",
    "    \"Attack code\", \"Significant flag\", \"Packet speed\", \"Data speed\", \"Avg packet len\",\n",
    "    \"Source IP count\", \"Time\"\n",
    "]\n",
    "\n",
    "event_columns = [\n",
    "    \"Attack ID\", \"Card\", \"Victim IP\", \"Port number\", \"Attack code\", \n",
    "    \"Detect count\", \"Significant flag\", \"Packet speed\", \"Data speed\", \n",
    "    \"Avg packet len\", \"Avg source IP count\", \"Start time\", \"End time\", \n",
    "    \"Whitelist flag\", \"Type\"\n",
    "]\n",
    "\n",
    "class DDoSDataset(Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.train_data_paths = [f'/home/appuser/data/train/SCLDDoS2024_SetA_events_extended.csv',\n",
    "                                 f'/home/appuser/data/train/SCLDDoS2024_SetB_events_extended.csv']\n",
    "        self.test_data_paths = [f'/home/appuser/data/test/SCLDDoS2024_SetC_events_extended.csv']     \n",
    "        \n",
    "        self.split = split   \n",
    "        \n",
    "        if split == 'train':\n",
    "            self.load_data(self.train_data_paths, apply_smote=False)\n",
    "        elif split == 'test':\n",
    "            self.load_data(self.test_data_paths, apply_smote=False)\n",
    "        else:\n",
    "            print(\"Invalid split. Use 'train' or 'test'\")\n",
    "            \n",
    "    \n",
    "    def get_ports(self):\n",
    "        return self.ddos_ports\n",
    "    \n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.features.numpy(), self.lables.numpy()\n",
    "    \n",
    "    def engineer_features_from_components(self, df_components):\n",
    "\n",
    "        grouped = df_components.groupby('Attack ID')\n",
    "\n",
    "        features = pd.DataFrame()\n",
    "\n",
    "        features['Unique Ports'] = grouped['Port number'].nunique()\n",
    "        features['Unique Victim IPs'] = grouped['Victim IP'].nunique()\n",
    "\n",
    "        return features.reset_index()\n",
    "    \n",
    "    # Function to infer attack codes for a full attack group\n",
    "    def infer_attack_code_row(self, row):\n",
    "        codes = set()\n",
    "\n",
    "        # CHARGEN:\n",
    "        if row[\"Packet speed\"] > 500000 and row[\"Data speed\"] > 400 and row[\"Port number\"] == 443:\n",
    "            codes.add(\"CHARGEN\")\n",
    "            \n",
    "        # CLDAP:\n",
    "        if row[\"Detect count\"] >= 10 and row[\"Data speed\"] > 400 and row[\"Port number\"] in [389, 53,80,443,0]:\n",
    "            codes.add(\"CLDAP\")\n",
    "            \n",
    "        # CoAP: I don't see any indicators for this\n",
    "        \n",
    "        # # DNS: notghing specific but we can use the port number\n",
    "        # if row[\"Port number\"] in [53, 443] and row[\"Data speed\"] < 30:\n",
    "        #     codes.add(\"DNS\")\n",
    "            \n",
    "        # Generic UDP:\n",
    "        if row[\"Data speed\"] < 20 and row[\"Port number\"] in [0,80, 56, 5656, 4500]:\n",
    "            codes.add(\"Generic UDP\")  \n",
    "            \n",
    "        # IPV4 fragmentation:\n",
    "        if row[\"Packet speed\"] > 1000000 and row[\"Data speed\"] > 1000 and row[\"Port number\"] in [0,80,443]:\n",
    "            codes.add(\"IPV4 fragmentation\")\n",
    "            \n",
    "        # NTP: I don't see anything\n",
    "        \n",
    "        # RDP: same\n",
    "        \n",
    "        # RPC: same\n",
    "        \n",
    "        # SNMP: same\n",
    "        \n",
    "        # SSDP: same\n",
    "        \n",
    "        # SYN Attack:\n",
    "        if row[\"Data speed\"] <= 10 and row[\"Avg packet len\"] <= 10 and row[\"Port number\"] in [80,11,22, 443, 0]:\n",
    "            codes.add(\"SYN Attack\")\n",
    "            \n",
    "        # Sentinel:\n",
    "        if row[\"Packet speed\"] < 30000 and row[\"Data speed\"] < 10 and row[\"Port number\"] == 0:\n",
    "            codes.add(\"Sentinel\")\n",
    "            \n",
    "        # TCP Anomaly:\n",
    "        if row[\"Avg packet len\"] == 0:\n",
    "            codes.add(\"TCP Anomaly\")\n",
    "\n",
    "        return \"; \".join(sorted(codes)) if codes else \"Unknown\"\n",
    "        \n",
    "    # preload the data as it makes the training much faster (and it easily fits in memory)\n",
    "    def load_data(self, data_paths, apply_smote=False, undersample=False, sample_factor=4, add_features=True):\n",
    "        data = []\n",
    "        component_data = []\n",
    "        \n",
    "        for path in data_paths:\n",
    "            event_df = pd.read_csv(path).fillna(0)\n",
    "            data.append(event_df)\n",
    "\n",
    "            # Attempt to load corresponding component file\n",
    "            comp_path = path.replace('_events_extended.csv', '_components.csv')\n",
    "            ref_event_path = path.replace('_events_extended.csv', '_events.csv')\n",
    "            if os.path.exists(comp_path) and add_features:\n",
    "                # Load event data\n",
    "                ref_ev_df = pd.read_csv(ref_event_path).fillna(0)\n",
    "                ref_ev_df.columns = event_columns\n",
    "\n",
    "                # Filter out invalid 'Attack ID's based on 'End time'\n",
    "                ref_ev_df2 = ref_ev_df[ref_ev_df['End time'].astype(str) != '0']\n",
    "                invalid_attack_ids = ref_ev_df[ref_ev_df['End time'].astype(str) == '0']['Attack ID'].unique()\n",
    "\n",
    "                # Filter the event data by removing rows with invalid 'Attack ID's\n",
    "                valid_attack_ids = ref_ev_df2['Attack ID'].unique()  # Attack IDs present in valid events\n",
    "\n",
    "                # Load component data\n",
    "                component_df = pd.read_csv(comp_path).fillna(0)\n",
    "                component_df.columns = component_columns\n",
    "\n",
    "                # Remove invalid attack IDs from component data\n",
    "                component_df = component_df[~component_df['Attack ID'].isin(invalid_attack_ids)]\n",
    "                \n",
    "                # Now filter component data to only include 'Attack ID's present in valid events\n",
    "                component_df = component_df[component_df['Attack ID'].isin(valid_attack_ids)]\n",
    "\n",
    "                # Append the filtered component data\n",
    "                component_data.append(component_df)\n",
    "            else:\n",
    "                print(f\"Component file not found: {comp_path}\")\n",
    "        \n",
    "        df = pd.concat(data, ignore_index=True)\n",
    "        \n",
    "        if component_data:\n",
    "            # df_components = pd.concat(component_data, ignore_index=True)\n",
    "            \n",
    "            # attack_id_to_code = (\n",
    "            #     df_components.groupby(\"Attack ID\")\n",
    "            #     .apply(self.infer_attack_code_group)\n",
    "            #     .rename(\"Inferred Attack Code\")\n",
    "            # )\n",
    "            \n",
    "            # Merge back the inferred attack code to all component rows\n",
    "            #df_components = df_components.merge(attack_id_to_code, on=\"Attack ID\")\n",
    "            \n",
    "            #comp_features = self.engineer_features_from_components(df_components)\n",
    "            df[\"Inferred Attack Code\"] = df.apply(self.infer_attack_code_row, axis=1)\n",
    "            # df = pd.concat([df, attack_id_to_code], axis=1)\n",
    "            # df = df.drop(columns=['Attack ID'])\n",
    "            cols = list(df.columns)\n",
    "            cols[-2], cols[-1] = cols[-1], cols[-2]\n",
    "            df = df[cols]\n",
    "            df = df.dropna(how='all')\n",
    "            \n",
    "        # Save the dataframes to a CSV file\n",
    "        if self.split == 'train':\n",
    "            df.to_csv('/home/appuser/data/train/A_B_inferred_attack_code.csv', index=False)\n",
    "        elif self.split == 'test':\n",
    "            df.to_csv('/home/appuser/data/test/C_inferred_attack_code.csv', index=False)\n",
    "       \n",
    "train = DDoSDataset('train')\n",
    "test = DDoSDataset('test')            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAT_TO_NUM_LABELS = {\n",
    "    \"Normal traffic\": 0,\n",
    "    \"Suspicious traffic\": 1,\n",
    "    \"DDoS attack\": 2,\n",
    "}\n",
    "\n",
    "component_columns = [\n",
    "    \"Attack ID\", \"Detect count\", \"Card\", \"Victim IP\", \"Port number\",\n",
    "    \"Attack code\", \"Significant flag\", \"Packet speed\", \"Data speed\", \"Avg packet len\",\n",
    "    \"Source IP count\", \"Time\"\n",
    "]\n",
    "\n",
    "event_columns = [\n",
    "    \"Attack ID\", \"Card\", \"Victim IP\", \"Port number\", \"Attack code\", \n",
    "    \"Detect count\", \"Significant flag\", \"Packet speed\", \"Data speed\", \n",
    "    \"Avg packet len\", \"Avg source IP count\", \"Start time\", \"End time\", \n",
    "    \"Whitelist flag\", \"Type\"\n",
    "]\n",
    "\n",
    "attack_code_ports = {\n",
    "    \"ACK Attack\": [],\n",
    "    \"CHARGEN\": [19],\n",
    "    \"CLDAP\": [389],\n",
    "    \"CoAP\": [5683],\n",
    "    \"DNS\": [53],\n",
    "    \"Generic UDP\": [],\n",
    "    \"IPV4 fragmentation\": [],\n",
    "    \"RDP\": [3389],\n",
    "    \"RPC\": [111, 135],\n",
    "    \"SNMP\": [161, 162],\n",
    "    \"SYN Attack\": [],\n",
    "    \"TCP Anomaly\": [],\n",
    "    \"NTP\": [123],\n",
    "    \"SSDP\": [1900],\n",
    "    \"Sentinel\": [],\n",
    "    \"Memcached\": [11211],\n",
    "    \"RIP\": [520],\n",
    "    \"TFTP\": [69],\n",
    "    \"WSD\": [3702],\n",
    "}\n",
    "\n",
    "# Port to attack mapping for fast lookup\n",
    "port_to_attack = {}\n",
    "for attack, ports in attack_code_ports.items():\n",
    "    for port in ports:\n",
    "        port_to_attack.setdefault(port, []).append(attack)\n",
    "\n",
    "class DDoSDataset(Dataset):\n",
    "    def __init__(self, split, use_inferred_atck_code):\n",
    "        self.train_data_paths = [f'/home/appuser/data/train/SCLDDoS2024_SetA_events_extended.csv',\n",
    "                                 f'/home/appuser/data/train/SCLDDoS2024_SetB_events_extended.csv']\n",
    "        self.test_data_paths = [f'/home/appuser/data/test/SCLDDoS2024_SetC_events_extended.csv']     \n",
    "        \n",
    "        self.inferred_train_data_paths = [f'/home/appuser/data/train/A_B_inferred_attack_code.csv']\n",
    "        self.inferred_test_data_paths = [f'/home/appuser/data/test/C_inferred_attack_code.csv']\n",
    "        \n",
    "        self.split = split   \n",
    "        self.use_inferred_atck_code = use_inferred_atck_code\n",
    "        \n",
    "        if split == 'train':\n",
    "            if use_inferred_atck_code:\n",
    "                self.features, self.lables = self.load_data(self.inferred_train_data_paths, apply_smote=False)\n",
    "            else:\n",
    "                self.features, self.lables = self.load_data(self.train_data_paths, apply_smote=False)            \n",
    "        elif split == 'test':\n",
    "            if use_inferred_atck_code:\n",
    "                self.features, self.lables = self.load_data(self.inferred_test_data_paths, apply_smote=False)\n",
    "            else:\n",
    "                self.features, self.lables = self.load_data(self.test_data_paths, apply_smote=False)\n",
    "        else:\n",
    "            print(\"Invalid split. Use 'train' or 'test'\")\n",
    "            \n",
    "    \n",
    "    def get_ports(self):\n",
    "        return self.ddos_ports\n",
    "    \n",
    "    \n",
    "    def get_data(self):\n",
    "        return self.features.numpy(), self.lables.numpy()\n",
    "        \n",
    "    # preload the data as it makes the training much faster (and it easily fits in memory)\n",
    "    def load_data(self, data_paths, apply_smote=False, undersample=False, sample_factor=4):\n",
    "        data = []\n",
    "        \n",
    "        for path in data_paths:\n",
    "            event_df = pd.read_csv(path).fillna(0)\n",
    "            data.append(event_df)\n",
    "\n",
    "            # Attempt to load corresponding component file\n",
    "            comp_path = path.replace('_events_extended.csv', '_components.csv')\n",
    "            ref_event_path = path.replace('_events_extended.csv', '_events.csv')\n",
    "        \n",
    "        df = pd.concat(data, ignore_index=True)\n",
    "            \n",
    "        \n",
    "        self.ddos_ports = df[df['Type'] == \"DDoS attack\"][\"Port number\"].unique()\n",
    "        \n",
    "        #feature_columns = df.columns[:19]  # All except the last column\n",
    "        \n",
    "        # comment this line I u need the frequency features\n",
    "        df = df.loc[:, ~df.columns.str.contains('Ac')]\n",
    "        feature_columns = df.columns[:-1]\n",
    "        label_column = df.columns[-1]  # The last column\n",
    "        \n",
    "        # Convert categorical labels to numeric using the dictionary\n",
    "        df[label_column] = df[label_column].map(CAT_TO_NUM_LABELS)\n",
    "        \n",
    "        # Check for missing or unknown labels\n",
    "        if df[label_column].isna().any():\n",
    "            print(df[label_column].isna().sum(), \"missing labels\")\n",
    "\n",
    "        X = df[feature_columns]\n",
    "        y = df[label_column]\n",
    "        \n",
    "        # one-hot encode the categorical features\n",
    "        if self.use_inferred_atck_code:\n",
    "            split_labels = X['Inferred Attack Code'].str.split('; ')\n",
    "            mlb = MultiLabelBinarizer()\n",
    "            attack_code_ohe = pd.DataFrame(mlb.fit_transform(split_labels),\n",
    "                                        columns=[f\"Inferred Attack Code_{cls}\" for cls in mlb.classes_],\n",
    "                                        index=X.index)\n",
    "\n",
    "            # Drop the original column and join the new one-hot encoded columns\n",
    "            X = X.drop(columns=['Inferred Attack Code'])\n",
    "            X = X.join(attack_code_ohe)\n",
    "            \n",
    "            \n",
    "            #X = pd.get_dummies(X, columns=[\"Inferred Attack Code\"], drop_first=False)        \n",
    "        \n",
    "        self.columns = X.columns\n",
    "        \n",
    "        cols = list(X.columns)\n",
    "        \n",
    "        \n",
    "        # Normalize the features\n",
    "        #features = self.normalize(features)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        if apply_smote and self.split == 'train':\n",
    "            class_counts = Counter(y)\n",
    "            sm = SMOTE(sampling_strategy={0: class_counts[0], 1:50000, 2:50000}, random_state=42)\n",
    "            sm = SMOTE(sampling_strategy='not majority', random_state=42)\n",
    "            sm = BorderlineSMOTE(sampling_strategy='not majority', random_state=42)\n",
    "            X_resampled, y_resampled = sm.fit_resample(X.values, y.values)\n",
    "            features = torch.tensor(X_resampled, dtype=torch.float32)\n",
    "            labels = torch.tensor(y_resampled, dtype=torch.long)\n",
    "        else:\n",
    "            features = torch.tensor(X.values, dtype=torch.float32)\n",
    "            labels = torch.tensor(y.values, dtype=torch.long)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if undersample and self.split == 'train':\n",
    "            # Undersample the majority class (label=0)\n",
    "            class_0_indices = np.where(labels.cpu().numpy() == 0)[0]\n",
    "            class_1_indices = np.where(labels.cpu().numpy() == 1)[0]\n",
    "            class_2_indices = np.where(labels.cpu().numpy() == 2)[0]\n",
    "\n",
    "            # Randomly undersample the majority class\n",
    "            num_class_0_samples = sample_factor*(len(class_1_indices) + len(class_2_indices))  # Same number as the minority class\n",
    "            class_0_indices_undersampled = np.random.choice(class_0_indices, num_class_0_samples, replace=False)\n",
    "\n",
    "            # Concatenate indices of class 1, 2, and undersampled class 0\n",
    "            undersampled_indices = np.concatenate([class_0_indices_undersampled, class_1_indices, class_2_indices])\n",
    "\n",
    "            # Subset the dataset to include only the sampled indices\n",
    "            features = features[undersampled_indices]\n",
    "            labels = labels[undersampled_indices]\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.lables[idx]\n",
    "\n",
    "        return x, y  # y is class index (long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DDoSDataset('test', use_inferred_atck_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>> Dataset.\n",
    "TaskType = Literal['regression', 'binclass', 'multiclass']\n",
    "\n",
    "# Regression.\n",
    "task_type: TaskType = 'multiclass'\n",
    "n_classes = None\n",
    "dataset = sklearn.datasets.fetch_california_housing()\n",
    "X_cont: np.ndarray = dataset['data']\n",
    "Y: np.ndarray = dataset['target']\n",
    "\n",
    "dataset = DDoSDataset(split='train', use_inferred_atck_code=True)\n",
    "X_cont = dataset.features.numpy()\n",
    "Y = dataset.lables.numpy()\n",
    "\n",
    "\n",
    "# Classification.\n",
    "n_classes = 3\n",
    "assert n_classes >= 2\n",
    "task_type: TaskType = 'binclass' if n_classes == 2 else 'multiclass'\n",
    "# X_cont, Y = sklearn.datasets.make_classification(\n",
    "#     n_samples=20000,\n",
    "#     n_features=8,\n",
    "#     n_classes=n_classes,\n",
    "#     n_informative=3,\n",
    "#     n_redundant=2,\n",
    "# )\n",
    "\n",
    "#task_is_regression = task_type == 'regression'\n",
    "\n",
    "# >>> Continuous features.\n",
    "X_cont: np.ndarray = X_cont.astype(np.float32)\n",
    "n_cont_features = X_cont.shape[1]\n",
    "\n",
    "# # >>> Categorical features.\n",
    "# # NOTE: the above datasets do not have categorical features, however,\n",
    "# # for the demonstration purposes, it is possible to generate them.\n",
    "# cat_cardinalities = [\n",
    "#     # NOTE: uncomment the two lines below to add two categorical features.\n",
    "#     # 4,  # Allowed values: [0, 1, 2, 3].\n",
    "#     # 7,  # Allowed values: [0, 1, 2, 3, 4, 5, 6].\n",
    "# ]\n",
    "# X_cat = (\n",
    "#     np.column_stack(\n",
    "#         [np.random.randint(0, c, (len(X_cont),)) for c in cat_cardinalities]\n",
    "#     )\n",
    "#     if cat_cardinalities\n",
    "#     else None\n",
    "# )\n",
    "\n",
    "# >>> Labels.\n",
    "if task_type == 'regression':\n",
    "    Y = Y.astype(np.float32)\n",
    "else:\n",
    "    assert n_classes is not None\n",
    "    Y = Y.astype(np.int64)\n",
    "    assert set(Y.tolist()) == set(\n",
    "        range(n_classes)\n",
    "    ), 'Classification labels must form the range [0, 1, ..., n_classes - 1]'\n",
    "\n",
    "# >>> Split the dataset.\n",
    "all_idx = np.arange(len(Y))\n",
    "train_idx, val_idx = sklearn.model_selection.train_test_split(\n",
    "    all_idx, train_size=0.8\n",
    ")\n",
    "\n",
    "data_numpy = {\n",
    "    'train': {'x_cont': X_cont[train_idx], 'y': Y[train_idx]},\n",
    "    'val': {'x_cont': X_cont[val_idx], 'y': Y[val_idx]},\n",
    "}\n",
    "\n",
    "s = 0\n",
    "# if X_cat is not None:\n",
    "#     data_numpy['train']['x_cat'] = X_cat[train_idx]\n",
    "#     data_numpy['val']['x_cat'] = X_cat[val_idx]\n",
    "#     data_numpy['test']['x_cat'] = X_cat[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the Dataset C as the testing class\n",
    "dataset = DDoSDataset(split='test', use_inferred_atck_code=True)\n",
    "X_cont = dataset.features.numpy()\n",
    "Y = dataset.lables.numpy()\n",
    "\n",
    "\n",
    "# Classification.\n",
    "n_classes = 3\n",
    "assert n_classes >= 2\n",
    "task_type: TaskType = 'binclass' if n_classes == 2 else 'multiclass'\n",
    "# X_cont, Y = sklearn.datasets.make_classification(\n",
    "#     n_samples=20000,\n",
    "#     n_features=8,\n",
    "#     n_classes=n_classes,\n",
    "#     n_informative=3,\n",
    "#     n_redundant=2,\n",
    "# )\n",
    "\n",
    "#task_is_regression = task_type == 'regression'\n",
    "\n",
    "# >>> Continuous features.\n",
    "X_cont: np.ndarray = X_cont.astype(np.float32)\n",
    "n_cont_features = X_cont.shape[1]\n",
    "\n",
    "# >>> Labels.\n",
    "if task_type == 'regression':\n",
    "    Y = Y.astype(np.float32)\n",
    "else:\n",
    "    assert n_classes is not None\n",
    "    Y = Y.astype(np.int64)\n",
    "    assert set(Y.tolist()) == set(\n",
    "        range(n_classes)\n",
    "    ), 'Classification labels must form the range [0, 1, ..., n_classes - 1]'\n",
    "    \n",
    "    \n",
    "data_numpy['test'] = {'x_cont': X_cont, 'y': Y}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature preprocessing.\n",
    "# NOTE\n",
    "# The choice between preprocessing strategies depends on a task and a model.\n",
    "\n",
    "# Simple preprocessing strategy.\n",
    "# preprocessing = sklearn.preprocessing.StandardScaler().fit(\n",
    "#     data_numpy[part]['x_cont'][:, continuous_feature_indices]\n",
    "# )\n",
    "\n",
    "# Advanced preprocessing strategy.\n",
    "# The noise is added to improve the output of QuantileTransformer in some cases.\n",
    "\n",
    "# own\n",
    "port_columns = [\n",
    "    'DNS', 'NTP', 'SNMP', 'SSDP', 'CLDAP', 'QUIC', 'RDP', 'CoAP', \n",
    "    'HTTP Flood', 'FTP', 'SSH', 'Memcached', 'WS-DD', 'NetBIOS', 'Kubernetes'\n",
    "]\n",
    "\n",
    "# gpt\n",
    "# port_columns = [\n",
    "#     'DNS', 'RDP', 'TCP', 'SYN', 'UDP', 'CoAP', 'Attack Ports'\n",
    "# ]\n",
    "\n",
    "continuous_feature_indices = [\n",
    "    i for i, col in enumerate(dataset.columns)\n",
    "    if not col.startswith(\"Inferred Attack Code_\")\n",
    "]\n",
    "\n",
    "\n",
    "X_cont_train_numpy = data_numpy['train']['x_cont'][:, continuous_feature_indices]\n",
    "# for testing the old model\n",
    "#X_cont_train_numpy = data_numpy['train']['x_cont']\n",
    "noise = (\n",
    "    np.random.default_rng(0)\n",
    "    .normal(0.0, 1e-5, X_cont_train_numpy.shape)\n",
    "    .astype(X_cont_train_numpy.dtype)\n",
    ")\n",
    "preprocessing = sklearn.preprocessing.QuantileTransformer(\n",
    "    n_quantiles=max(min(len(train_idx) // 30, 1000), 10),\n",
    "    output_distribution='normal',\n",
    "    subsample=10**9,\n",
    ").fit(X_cont_train_numpy + noise)\n",
    "del X_cont_train_numpy\n",
    "\n",
    "# for the old model\n",
    "# Apply the preprocessing to the training, test and validation sets.\n",
    "# for part in data_numpy:\n",
    "#     # Transform only continuous features\n",
    "#     data_numpy[part]['x_cont'] = preprocessing.transform(\n",
    "#         data_numpy[part]['x_cont']\n",
    "#     )\n",
    "#     # Leave port features unchanged\n",
    "\n",
    "# Apply the preprocessing to the training, test and validation sets.\n",
    "for part in data_numpy:\n",
    "    # Transform only continuous features\n",
    "    data_numpy[part]['x_cont'][:, continuous_feature_indices] = preprocessing.transform(\n",
    "        data_numpy[part]['x_cont'][:, continuous_feature_indices]\n",
    "    )\n",
    "    # Leave port features unchanged\n",
    "\n",
    "# Label preprocessing.\n",
    "class RegressionLabelStats(NamedTuple):\n",
    "    mean: float\n",
    "    std: float\n",
    "\n",
    "\n",
    "Y_train = data_numpy['train']['y'].copy()\n",
    "if task_type == 'regression':\n",
    "    # For regression tasks, it is highly recommended to standardize the training labels.\n",
    "    regression_label_stats = RegressionLabelStats(\n",
    "        Y_train.mean().item(), Y_train.std().item()\n",
    "    )\n",
    "    Y_train = (Y_train - regression_label_stats.mean) / regression_label_stats.std\n",
    "else:\n",
    "    regression_label_stats = None\n",
    "    \n",
    "regression_label_stats = None\n",
    "Y_train = data_numpy['train']['y'].copy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PyTorch settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:        CUDA\n",
      "AMP:           False (dtype: torch.bfloat16)\n",
      "torch.compile: False\n"
     ]
    }
   ],
   "source": [
    "regression_label_stats = None\n",
    "Y_train = data_numpy['train']['y'].copy()\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Convert data to tensors\n",
    "data = {\n",
    "    part: {k: torch.as_tensor(v, device=device) for k, v in data_numpy[part].items()}\n",
    "    for part in data_numpy\n",
    "}\n",
    "Y_train = torch.as_tensor(Y_train, device=device)\n",
    "if task_type == 'regression':\n",
    "    for part in data:\n",
    "        data[part]['y'] = data[part]['y'].float()\n",
    "    Y_train = Y_train.float()\n",
    "\n",
    "# Automatic mixed precision (AMP)\n",
    "# torch.float16 is implemented for completeness,\n",
    "# but it was not tested in the project,\n",
    "# so torch.bfloat16 is used by default.\n",
    "amp_dtype = (\n",
    "    torch.bfloat16\n",
    "    if torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "    else torch.float16\n",
    "    if torch.cuda.is_available()\n",
    "    else None\n",
    ")\n",
    "# Changing False to True will result in faster training on compatible hardware.\n",
    "amp_enabled = False and amp_dtype is not None\n",
    "grad_scaler = torch.cuda.amp.GradScaler() if amp_dtype is torch.float16 else None  # type: ignore\n",
    "\n",
    "# torch.compile\n",
    "compile_model = False\n",
    "\n",
    "# fmt: off\n",
    "print(\n",
    "    f'Device:        {device.type.upper()}'\n",
    "    f'\\nAMP:           {amp_enabled} (dtype: {amp_dtype})'\n",
    "    f'\\ntorch.compile: {compile_model}'\n",
    ")\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose one of the two configurations below.\n",
    "\n",
    "# Define the model\n",
    "arch_type = 'tabm'\n",
    "bins = None\n",
    "\n",
    "# arch_type = 'tabm-mini'\n",
    "# bins = rtdl_num_embeddings.compute_bins(data['train']['x_cont'])\n",
    "\n",
    "model = Model(\n",
    "    n_num_features=data['train']['x_cont'].shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    n_classes=n_classes,\n",
    "    backbone={\n",
    "        'type': 'MLP',\n",
    "        'n_blocks': 3 if bins is None else 2,\n",
    "        'd_block': 256,\n",
    "        'dropout': 0.2,\n",
    "        'n_blocks': 5\n",
    "    },\n",
    "    bins=bins,\n",
    "    num_embeddings=(\n",
    "        None\n",
    "        if bins is None\n",
    "        else {\n",
    "            'type': 'PiecewiseLinearEmbeddings',\n",
    "            'd_embedding': 16,\n",
    "            'activation': False,\n",
    "            'version': 'B',\n",
    "        }\n",
    "    ),\n",
    "    arch_type=arch_type,\n",
    "    k=48,\n",
    "    share_training_batches=True,\n",
    ").to(device)\n",
    "optimizer = torch.optim.AdamW(make_parameter_groups(model), lr=1e-3, weight_decay=3e-4)\n",
    "# regi: 1e-3\n",
    "if compile_model:\n",
    "    # NOTE\n",
    "    # `torch.compile` is intentionally called without the `mode` argument\n",
    "    # (mode=\"reduce-overhead\" caused issues during training with torch==2.0.1).\n",
    "    model = torch.compile(model)\n",
    "    evaluation_mode = torch.no_grad\n",
    "else:\n",
    "    evaluation_mode = torch.inference_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score before training: 0.0054\n"
     ]
    }
   ],
   "source": [
    "@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
    "def apply_model(part: str, idx: Tensor) -> Tensor:\n",
    "    return (\n",
    "        model(\n",
    "            data[part]['x_cont'][idx],\n",
    "            data[part]['x_cat'][idx] if 'x_cat' in data[part] else None,\n",
    "        )\n",
    "        .squeeze(-1)  # Remove the last dimension for regression tasks.\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "\n",
    "base_loss_fn = F.mse_loss if task_type == 'regression' else F.cross_entropy\n",
    "#weight = torch.tensor([1.0, 1.0, 1.0], device=device)\n",
    "\n",
    "\n",
    "def loss_fn(y_pred: Tensor, y_true: Tensor, weight: float = None) -> Tensor:\n",
    "    # TabM produces k predictions. Each of them must be trained separately.\n",
    "    # (regression)     y_pred.shape == (batch_size, k)\n",
    "    # (classification) y_pred.shape == (batch_size, k, n_classes)\n",
    "    k = y_pred.shape[-1 if task_type == 'regression' else -2]\n",
    "    return base_loss_fn(\n",
    "        y_pred.flatten(0, 1),\n",
    "        y_true.repeat_interleave(k) if model.share_training_batches else y_true,\n",
    "        weight=weight,\n",
    "    )\n",
    "\n",
    "\n",
    "@evaluation_mode()\n",
    "def evaluate(part: str) -> float:\n",
    "    model.eval()\n",
    "\n",
    "    # When using torch.compile, you may need to reduce the evaluation batch size.\n",
    "    eval_batch_size = 8096\n",
    "    y_pred: np.ndarray = (\n",
    "        torch.cat(\n",
    "            [\n",
    "                apply_model(part, idx)\n",
    "                for idx in torch.arange(len(data[part]['y']), device=device).split(\n",
    "                    eval_batch_size\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    if task_type == 'regression':\n",
    "        # Transform the predictions back to the original label space.\n",
    "        assert regression_label_stats is not None\n",
    "        y_pred = y_pred * regression_label_stats.std + regression_label_stats.mean\n",
    "\n",
    "    # Compute the mean of the k predictions.\n",
    "    if task_type != 'regression':\n",
    "        # For classification, the mean must be computed in the probabily space.\n",
    "        y_pred = scipy.special.softmax(y_pred, axis=-1)\n",
    "    y_pred = y_pred.mean(1)\n",
    "\n",
    "    y_true = data[part]['y'].cpu().numpy()\n",
    "    score = (\n",
    "        -(sklearn.metrics.mean_squared_error(y_true, y_pred) ** 0.5)\n",
    "        if task_type == 'regression'\n",
    "        else sklearn.metrics.f1_score(y_true, y_pred.argmax(1), average='macro')\n",
    "    )\n",
    "    return float(score)  # The higher -- the better.\n",
    "\n",
    "\n",
    "print(f'Test score before training: {evaluate(\"test\"):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.7510 (test) 0.7353\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.7890 (test) 0.7722\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8129 (test) 0.7184\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8158 (test) 0.7351\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 112.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8310 (test) 0.7617\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8326 (test) 0.7475\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8339 (test) 0.7761\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8320 (test) 0.7491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8317 (test) 0.7448\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8361 (test) 0.7882\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8328 (test) 0.7286\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8317 (test) 0.7568\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8321 (test) 0.8173\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8312 (test) 0.7319\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8367 (test) 0.7311\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8388 (test) 0.8355\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8428 (test) 0.7643\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8428 (test) 0.7677\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8437 (test) 0.7368\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8384 (test) 0.7439\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8383 (test) 0.7483\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8452 (test) 0.7925\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8388 (test) 0.7670\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8439 (test) 0.7988\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8408 (test) 0.7604\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8432 (test) 0.7915\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8516 (test) 0.8294\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8492 (test) 0.7582\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8440 (test) 0.8307\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8516 (test) 0.7951\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 112.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8481 (test) 0.7924\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8425 (test) 0.7912\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8450 (test) 0.8147\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8512 (test) 0.8388\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8567 (test) 0.8242\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8448 (test) 0.7679\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8516 (test) 0.8233\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8474 (test) 0.7501\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8555 (test) 0.8175\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8550 (test) 0.8118\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8512 (test) 0.8013\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8411 (test) 0.7858\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8429 (test) 0.7799\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8489 (test) 0.8536\n",
      "ðŸŒ¸ New best epoch! ðŸŒ¸\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 114.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8518 (test) 0.8118\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8466 (test) 0.7712\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 115.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8401 (test) 0.7478\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8499 (test) 0.8401\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8558 (test) 0.8157\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8520 (test) 0.8052\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8561 (test) 0.8045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8482 (test) 0.8045\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8507 (test) 0.8222\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8519 (test) 0.7936\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 112.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8523 (test) 0.8218\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 110.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8485 (test) 0.8004\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 111.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8529 (test) 0.7683\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 109.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8554 (test) 0.7767\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 107.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8525 (test) 0.8025\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 110.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8548 (test) 0.7514\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 108.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8457 (test) 0.7769\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 109.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8576 (test) 0.7782\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8517 (test) 0.8058\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 112.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8538 (test) 0.8206\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8562 (test) 0.8032\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 828/828 [00:07<00:00, 113.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(val) 0.8540 (test) 0.7945\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 777/828 [00:06<00:00, 112.55it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m tqdm(batches, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     44\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 45\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(apply_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, batch_idx), Y_train[batch_idx], weight\u001b[38;5;241m=\u001b[39mloss_weight)\n\u001b[1;32m     47\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/optim/optimizer.py:820\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    818\u001b[0m     per_device_and_dtype_grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 820\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_zero_grad_profile_name):\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_groups:\n\u001b[1;32m    822\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/profiler.py:622\u001b[0m, in \u001b[0;36mrecord_function.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m--> 622\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprofiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_record_function_exit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_RecordFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    624\u001b[0m     torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39m_record_function_exit(record)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_ops.py:591\u001b[0m, in \u001b[0;36mOpOverload.__call__\u001b[0;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<OpOverload(op=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, overload=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    588\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m::\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_overloadname\n\u001b[1;32m    589\u001b[0m     )\n\u001b[0;32m--> 591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[1;32m    592\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;66;03m# are named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Tensorboard for training\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "run_name = \"tabm_no_frequency_features_inferred_attack_code\"\n",
    "tb_log_dir = f'/home/appuser/src/logs/TabM/{run_name}'\n",
    "writer = SummaryWriter(log_dir=tb_log_dir)\n",
    "\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_weight = torch.tensor([1.0, 1.0, 3.0], device=device)\n",
    "\n",
    "train_size = len(train_idx)\n",
    "batch_size = 256\n",
    "epoch_size = math.ceil(train_size / batch_size)\n",
    "best = {\n",
    "    'val': -math.inf,\n",
    "    'test': -math.inf,\n",
    "    'epoch': -1,\n",
    "}\n",
    "# Early stopping: the training stops when\n",
    "# there are more than `patience` consequtive bad updates.\n",
    "patience = 1000\n",
    "remaining_patience = patience\n",
    "\n",
    "\n",
    "\n",
    "print('-' * 88 + '\\n')\n",
    "for epoch in range(n_epochs):\n",
    "    batches = (\n",
    "        torch.randperm(train_size, device=device).split(batch_size)\n",
    "        if model.share_training_batches\n",
    "        else [\n",
    "            x.transpose(0, 1).flatten()\n",
    "            for x in torch.rand((model.k, train_size), device=device)\n",
    "            .argsort(dim=1)\n",
    "            .split(batch_size, dim=1)\n",
    "        ]\n",
    "    )\n",
    "    epoch_loss = 0.0\n",
    "    epoch_num = 0\n",
    "    for batch_idx in tqdm(batches, desc=f'Epoch {epoch}'):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(apply_model('train', batch_idx), Y_train[batch_idx], weight=loss_weight)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_num += 1\n",
    "        if grad_scaler is None:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            grad_scaler.scale(loss).backward()  # type: ignore\n",
    "            grad_scaler.step(optimizer)\n",
    "            grad_scaler.update()\n",
    "\n",
    "    val_score = evaluate('val')\n",
    "    test_score = evaluate('test')\n",
    "    writer.add_scalar('validation_macro_F1', val_score, epoch+1)\n",
    "    writer.add_scalar('test_macro_F1', test_score, epoch+1)\n",
    "    writer.add_scalar('train_loss', epoch_loss/epoch_num, epoch+1)\n",
    "    print(f'(val) {val_score:.4f} (test) {test_score:.4f}')\n",
    "\n",
    "    if test_score > best['test']:\n",
    "        print('ðŸŒ¸ New best epoch! ðŸŒ¸')\n",
    "        best = {'val': val_score, 'test': test_score, 'epoch': epoch}\n",
    "        remaining_patience = patience\n",
    "        torch.save(model.state_dict(), f\"{run_name}.pth\")\n",
    "    else:\n",
    "        remaining_patience -= 1\n",
    "\n",
    "    if remaining_patience < 0:\n",
    "        break\n",
    "\n",
    "    print()\n",
    "    if (epoch+1) % 30 == 0:\n",
    "        loss_weight += torch.tensor([0.0, 0.0, 0.0], device=device)\n",
    "\n",
    "\n",
    "print('\\n\\nResult:')\n",
    "print(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREV BEST SETUP\n",
    "\n",
    "# Define the model\n",
    "arch_type = 'tabm'\n",
    "bins = None\n",
    "\n",
    "# arch_type = 'tabm-mini'\n",
    "# bins = rtdl_num_embeddings.compute_bins(data['train']['x_cont'])\n",
    "\n",
    "model = Model(\n",
    "    n_num_features=data['train']['x_cont'].shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    n_classes=n_classes,\n",
    "    backbone={\n",
    "        'type': 'MLP',\n",
    "        'n_blocks': 3 if bins is None else 2,\n",
    "        'd_block': 256,\n",
    "        'dropout': 0.2,\n",
    "        'n_blocks': 5\n",
    "    },\n",
    "    bins=bins,\n",
    "    num_embeddings=(\n",
    "        None\n",
    "        if bins is None\n",
    "        else {\n",
    "            'type': 'PiecewiseLinearEmbeddings',\n",
    "            'd_embedding': 16,\n",
    "            'activation': False,\n",
    "            'version': 'B',\n",
    "        }\n",
    "    ),\n",
    "    arch_type=arch_type,\n",
    "    k=48,\n",
    "    share_training_batches=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DYNMIC WEIHGT SETUP\n",
    "arch_type = 'tabm'\n",
    "bins = None\n",
    "\n",
    "# TabM-mini with the piecewise-linear embeddings.\n",
    "# arch_type = 'tabm-mini'\n",
    "# bins = rtdl_num_embeddings.compute_bins(data['train']['x_cont'])\n",
    "\n",
    "# arch_type = 'tabm-packed'\n",
    "# bins = rtdl_num_embeddings.compute_bins(data['train']['x_cont'])\n",
    "\n",
    "# d_block: 512\n",
    "# n_blocks: 3\n",
    "\n",
    "model = Model(\n",
    "    n_num_features=data['train']['x_cont'].shape[1],\n",
    "    cat_cardinalities=[],\n",
    "    n_classes=n_classes,\n",
    "    backbone={\n",
    "        'type': 'MLP',\n",
    "        'n_blocks': 3 if bins is None else 2,\n",
    "        'd_block': 1024,\n",
    "        'dropout': 0.05,\n",
    "        'n_blocks': 2\n",
    "    },\n",
    "    bins=bins,\n",
    "    num_embeddings=(\n",
    "        None\n",
    "        if bins is None\n",
    "        else {\n",
    "            'type': 'PiecewiseLinearEmbeddings',\n",
    "            'd_embedding': 32,\n",
    "            'activation': False,\n",
    "            'version': 'B',\n",
    "        }\n",
    "    ),\n",
    "    arch_type=arch_type,\n",
    "    k=48,\n",
    "    share_training_batches=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']['x_cont'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.autocast(device.type, enabled=amp_enabled, dtype=amp_dtype)  # type: ignore[code]\n",
    "def apply_model(part: str, idx: Tensor) -> Tensor:\n",
    "    return (\n",
    "        model(\n",
    "            data[part]['x_cont'][idx],\n",
    "            data[part]['x_cat'][idx] if 'x_cat' in data[part] else None,\n",
    "        )\n",
    "        .squeeze(-1)  # Remove the last dimension for regression tasks.\n",
    "        .float()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 479392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10834 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10834/10834 [00:07<00:00, 1425.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9906\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score      support\n",
      "0              0.996434  0.994424  0.995428  125892.0000\n",
      "1              0.873532  0.950524  0.910403    3052.0000\n",
      "2              0.659615  0.650237  0.654893    1055.0000\n",
      "accuracy       0.990600  0.990600  0.990600       0.9906\n",
      "macro avg      0.843194  0.865062  0.853575  129999.0000\n",
      "weighted avg   0.990815  0.990600  0.990668  129999.0000\n",
      "\n",
      "F1 (Micro): 0.9906\n",
      "F1 (Macro): 0.8536\n",
      "\n",
      "Class-wise Accuracy (Recall):\n",
      " 0    0.994424\n",
      "1    0.950524\n",
      "2    0.650237\n",
      "Name: recall, dtype: float64\n",
      "\n",
      "Confusion Matrix:\n",
      " [[125190    399    303]\n",
      " [   100   2901     51]\n",
      " [   348     21    686]]\n"
     ]
    }
   ],
   "source": [
    "from time import time   \n",
    "\n",
    "# Inference on the test dataset\n",
    "#model.load_state_dict(torch.load('/home/appuser/src/visualization/tabm_no_frequ_features_extended_more_weight.pth'))\n",
    "model.load_state_dict(torch.load(f\"{run_name}.pth\"))\n",
    "model.eval()\n",
    "\n",
    "part = \"test\"\n",
    "\n",
    "eval_batch_size = 12\n",
    "# y_pred: np.ndarray = (\n",
    "#     torch.cat(\n",
    "#         [\n",
    "#             apply_model(part, idx).cpu()\n",
    "#             for idx in torch.arange(len(data[part]['y']), device=device).split(\n",
    "#                 eval_batch_size\n",
    "#             )\n",
    "#         ]\n",
    "#     )\n",
    "#     .numpy()\n",
    "# )\n",
    "\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {num_params}\")\n",
    "\n",
    "# model.cpu()\n",
    "\n",
    "# single_sample = data[part]['x_cont'][0].unsqueeze(0)\n",
    "# for single_sample in data[part]['x_cont']:\n",
    "#     single_sample = single_sample.unsqueeze(0)\n",
    "#     start = time()\n",
    "#     preds = model(single_sample.cpu())\n",
    "#     duration = (time() - start)*10**3\n",
    "#     print(f\"Time taken for single sample: {duration:.2f} ms\")\n",
    "\n",
    "y_pred_list = []\n",
    "for idx in tqdm(torch.arange(len(data[part]['y']), device=device).split(eval_batch_size)):\n",
    "    with torch.no_grad():\n",
    "        preds = apply_model(part, idx).cpu()\n",
    "        probs = scipy.special.softmax(preds.numpy(), axis=-1)\n",
    "        averaged = probs.mean(1)  # shape: [B, C]\n",
    "        preds_class = np.argmax(averaged, axis=1)\n",
    "        y_pred_list.append(preds_class)\n",
    "\n",
    "y_pred = np.concatenate(y_pred_list)\n",
    "\n",
    "y_test = data[part]['y'].cpu().numpy()\n",
    "\n",
    "# Overall accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Classification report (includes precision, recall, F1 per class + macro/micro)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "print(\"\\nClassification Report:\\n\", report_df)\n",
    "\n",
    "# F1 scores\n",
    "f1_micro = f1_score(y_test, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "print(f\"\\nF1 (Micro): {f1_micro:.4f}\")\n",
    "print(f\"F1 (Macro): {f1_macro:.4f}\")\n",
    "\n",
    "# Class-wise accuracy (same as recall per class)\n",
    "class_wise_accuracy = report_df.loc[[str(i) for i in np.unique(y_test)], \"recall\"]\n",
    "print(\"\\nClass-wise Accuracy (Recall):\\n\", class_wise_accuracy)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
